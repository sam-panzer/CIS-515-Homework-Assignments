\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath}

\setcounter{secnumdepth}{0}
\begin{document}

\begin{center}CIS 515 --- HW1\\Sam Panzer and Seth Shannin\end{center}
\subsection{Problem B1}
  Suppose $v \in V,$ since $V$ is nonempty. Then $0 = v + (-v)$, where $-v$ is $v$'s additive inverse.
\begin{itemize}
  \item $\alpha \cdot 0 = 0$:\\
    $\alpha\cdot 0 = \alpha\cdot(0 + 0) = \alpha\cdot 0 + \alpha\cdot 0,$ which
    implies that $\alpha\cdot 0 = 0.$
  \item $0\cdot v = 0$: \\
    $0 \cdot v = (0 + 0)\cdot v = 0 \cdot v + 0 \cdot v,$ which again implies
    that $0 \cdot v = 0.$
  \item $\alpha \cdot (-v) = -(\alpha \cdot v) = (-\alpha) \cdot v$:\\
  $\alpha \cdot (-v) + \alpha \cdot v = \alpha ((-v) + v) = \alpha \cdot 0 = 0$,
  thus showing that $\alpha \cdot (-v) =  -(\alpha \cdot v).$ Similarly,
  $(-\alpha) \cdot v + \alpha \cdot v = (-\alpha + \alpha)\cdot v =
  0 \cdot v = 0$.
  \end{itemize}

\subsection{Problem B2}
\subsubsection{(1)}
Let $U = (u_1,\dots,u_m)$ and $V = (u_1,\dots,u_m)$ be two families of
vectors in $E,$ such that $u$ is linearly independent, $v = Au,$ and $A$
is a square matrix with a nonzero trace.
Suppose, with the goal of showing that $\lambda_i = 0\, \forall i,$ that 
$\displaystyle{\sum_{i=1}^\infty \lambda_i v_i = 0}.$ This is equivalent to
\[ \sum_{i=1}^m\lambda_i\left( \sum_{j=1}^m a_{i,j}u_j \right) = 
   \sum_{j=1}^m u_j\left( \sum_{i=1}^m \lambda_i a_{i,j} \right) = 0.
\]
Since $U$ is linearly independent, this implies that the inner sum is zero for
each $j.$ That is,
$\displaystyle{ \sum_{i=1}^m \lambda_i a_{i,j} = 0, \,\forall j }$
Since $a_{i,j} = 0$ for all $i > j,$ all terms with $j < i$ are zero. We can
rewrite this sum as
$\displaystyle{ \sum_{i=1}^j \lambda_i a_{i,j} = 0, \,\forall j }$

We can now show by induction on $j$ that $\lambda_j = 0$.
In the base case, $j = 1,$ so the sum evaluates to $\lambda_1 \cdot a_{1,1}$.
Since $a_{k,k} \neq 0\, \forall k$ by assumption, $\lambda_1 = 0$.

For the inductive case, we assume that $\lambda_k = 0\, \forall k < j.$ Thus
the sum evaluates to $\lambda_j a_{j,j} = 0$ which implies that $\lambda_j=0$.
We conclude that $\lambda_j = 0$ for all $1 \leq j \leq m,$ which implies that
$U$ is linearly independent, as desired.

\subsubsection{(2)}
$A$ is an $m\times m$ upper-triangular matrix with entries $a_{i,j},$ and the proof proceeds
by induction on $m$.

The base case is trivial, since a square matrix with one nonzero entry is, by
definition, upper-triangular. The inverse is $(a_{1,1}^{-1}),$ which clearly
gives $AA^{-1} = I.$

For the inductive case, suppose that 
\[A=\left(
\begin{array}{ccc|c}
& && \\
&B&&v\\
& && \\
\hline
&0&&r
\end{array}
\right)\]
where $B$ is some upper-triangular matrix, $v$ is a column vector with $m-1$
elements, and $r$ is nonzero. The induction hypothesis tells us that $B^{-1}$
exists and is upper-triangular. We construct the matrix $A^{-1} = (z_{i,j})$
as follows:
When $i,j < m)$, we assign $z_{i,j} = B^{-1}.$ We set $z_{m,j}=0$ for $j < m$,
and $z_{m,m} = r^{-1}.$ The last bit to work out is the column vector that
occupies the same region as $v.$
We now have
\[A^{-1}=\left(
\begin{array}{ccc|c}
& && \\
&B^{-1}&&u\\
& && \\
\hline
&0&&r^{-1}
\end{array}
\right)\]

Now, we check to see that we actually have the correct inverse.
\[z_{i,j} = \sum_{k=1}^m a_{i,k}z_{k,j} \]
We need $z_{i,j} = 1$ when $i \neq j$ and $z_{i,j} = 0$ otherwise.


\subsubsection{(3)}
This follows as a direct consequence of (1) and (2). We only need to show that
$V$ being linearly independent implies that $U$ is linearly independent, since
(1) covers the converse. And if $U$ is linearly independent, then we can write
$V$ as linear combinations of elements of $U$. This is because we assume that
$V = AU,$ and knowing that $A$ is invertible lets us rewrite this as 
$A^{-1}V = U$.
Then we can apply (1) again to find that $U$ is linearly independent.

\subsection{Problem B3}
\subsubsection{(1)}
Each linear equation gives us $x_k + 2x_{k+1} = b_k$ when $k < n,$ and
$x_n = b_n$ from the case $k = n.$ We can solve the first equation to give us
$x_k = b_k - 2x_{k+1},$ which has the nice summation-form solution of
\[x_k  = \sum_{i=k}^nb_{k+i}(-2)^i\]

\subsubsection{(2)}
We already proved this in Problem B2(2)!
Actually, the inverse is quite nice. Each diagonal (\emph{i.e.} the set
${a_{i,j}|j-i=k}$) has the same value, with value $(-2)^k$ ($k$ is the vertical
displacement from the diagonal).
Of course when $n=300$, the upper right-hand corner would be equal to
$-2^{299},$ which is too large to compute, since the universe doesn't have
enough atoms!

\subsubsection{(3)}
We will show by induction on $m$ that $(A-I)^m$ has $m$ diagonals that are equal
to 0. That is, if $(A-I)^m = (z_{i,j}),$ then $z_{i,j} = 0$ when $i - j < m$,
and when $m \geq n$ the result is just 0.
The base case is trivially satisfied, since $(A-I)$ has zeros down the main
diagonal, and it's lower-triangular.

The inductive case assumes that $(z_{i,j}) = (A-I)^m$ satisfies 
$z_{i,j} = 0$ when $i - j < m$.
We consider $z'_{i,j} = (A-I)^m(A-I).$
By the definition of matrix multiplication, 
\[z'_{i,j} = \sum_{k=0}^m ((A-I)^m)_{i,k}(A-I)_{k,j}\]
Now suppose that $i - j < m + 1.$
Notice that $((A - I)^m)_{i,k} = 0$ for $i - k < m$, which can be rewritten as
$i - m < k$
Similarly, $(A - I)_{k,j} = 0$ for $k < j + 1$
Thus one of the two terms of the summation is zero when for $i - m < k$ and
for $k < j + 1$, which is the same as $i - m < j + 1$, or $i - j < m + 1$.
So $z_{i,j}$ is the sum of $n$ zeros when $i - m < j + 1$, as desired.

\subsection{Problem B4}
\subsubsection{(1)}
$B^2_0(t) = (1-t)^2 = 1 - 2t + t^2 = (1,-2,1)\cdot(1, t, t^2)$\\
$B^2_1(t) = 2(1-t)t = 2t - 2t^2 = (0,2,-2)\cdot(1, t, t^2)$\\
$B^2_1(t) = t^2 = (0,0,1) \cdot(1,t,t^2)$\\
And $B^2_0(t) + B^2_1(t) + B^2_2(t) = (1 - 2t + t^2) + (2t - 2t^2) + t^2 = 1.$

\subsubsection{(2)}
$B^3_0(t) = (1-t)^3 = 1 - 3t + 3t^2 - t^3 = (1,-3,3,-1)\cdot(1, t, t^2,t^3)$\\
$B^3_1(t) = 3(1-t)^2t = 3t - 6t^2 + 3t^3 = (0,3,-6,3)\cdot(1, t, t^2,t^3)$\\
$B^3_2(t) = 3(1-t)t^2 = 3t^2 -3t^3 = (0,0,3,-3)\cdot(1, t, t^2,t^3)$\\
$B^3_3(t) = t^3 = t^3 = (0,0,0,1)\cdot(1, t, t^2,t^3)$\\
And $B^3_0(t) + B^3_1(t) + B^3_2(t) + B^3_3(t) = (1 - 3t + 3t^2 - t^3) + (3t -
6t^2 + 3t^3) + (3t^2 -3t^3) + (t^3) = 1$

\subsection{(3)}
Question B4(2) shows the preconditions needed to apply B2(1), once we show that
$(1,t,t^2)$ and $(1,t,t^2,t^3)$ are each linearly independent. This follows
because they each clearly span the polynomials of respective maximum degree 2
and 3. Since these families are the minimal spanning families, they must be
bases and therefore linearly independent.

\subsection{(4)}
This follows from the Binomial Identity, which states that
\[(x + y)^n = \sum_{j=k}^n \binom{n}{m}x^{n-k}y^k\]
Then 
\[(1-t)^{n-k} = \sum_{j=0}^{n-k}\binom{n-k}{j}1^{n-k-j}(-1)^jx^j =
\sum_{j=0}^{n-k}\binom{n-k}{j}1^{n-k-j}(-1)^jx^j \]
Multiplying by $t^k$ gives
$(1-t)^{n-k}t^k = \displaystyle{\sum_{j=0}^{n-k}
\binom{n-k}{j}(-1)^jx^{j+k}}$
Then we can shift indices by $k$, replacing $j$ with $j-k$ to yield
\[(1-t)^{n-k}t^k = \sum_{j=k}^{n} \binom{n-k}{j-k}(-1)^{j-k}x^j\]
Then, we can multiply by $\binom{n}{k}$ to obtain
\[\binom{n}{k}(1-t)^{n-k}t^k = \sum_{j=k}^{n} \binom{n}{k}\binom{n-k}{j-k}(-1)^{j-k}x^j\]
Now, we just need to apply the identity $\binom{n}{k}\binom{n-k}{j-k} =
\binom{n}{j}\binom{j}{k}$ to see that 
\[\binom{n}{k}(1-t)^{n-k}t^k = \sum_{j=k}^{n} \binom{n}{j}\binom{j}{k}(-1)^{j-k}x^j\]

A proof of this last identity is as follows:
\[\binom{n}{k}\binom{n-k}{j-k} = \frac{n!(n-k)!}{k!(n-k)!(j-k)!(n-j)!}
=\frac{n!}{j!(n-j)!}\frac{j!}{k!(j-k)!} = \binom{n}{j}\binom{j}{k}\]

\subsection{(5)}

\subsection{Problem B5}
\subsubsection{(1)}
Since $A$ is invertible, we know that $A^{1}$ exists.
We then assume that $AX = 0.$ We can left-multiply by $A^{-1},$ giving us
$A^{-1}AX =  A^{-1}0$, or $X = 0,$ as desired.

\subsubsection{(2)}

\end{document}
