\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath}

\setcounter{secnumdepth}{0}
\begin{document}

\begin{center}CIS 515 --- HW1\\Sam Panzer and Kevin Shi\end{center}
\subsection{Problem B1}
  Suppose $v \in V,$ since $V$ is nonempty. Then $0 = v + (-v)$, where $-v$ is $v$'s additive inverse.
\begin{itemize}
  \item $\alpha \cdot 0 = 0$:\\
    $\alpha\cdot 0 = \alpha\cdot(0 + 0) = \alpha\cdot 0 + \alpha\cdot 0,$ which
    implies that $\alpha\cdot 0 = 0.$
  \item $0\cdot v = 0$: \\
    $0 \cdot v = (0 + 0)\cdot v = 0 \cdot v + 0 \cdot v,$ which again implies
    that $0 \cdot v = 0.$
  \item $\alpha \cdot (-v) = -(\alpha \cdot v) = (-\alpha) \cdot v$:\\
  $\alpha \cdot (-v) + \alpha \cdot v = \alpha ((-v) + v) = \alpha \cdot 0 = 0$,
  thus showing that $\alpha \cdot (-v) =  -(\alpha \cdot v).$ Similarly,
  $(-\alpha) \cdot v + \alpha \cdot v = (-\alpha + \alpha)\cdot v =
  0 \cdot v = 0$.
  \end{itemize}

\subsection{Problem B2}
\subsubsection{(1)}
Let $U = (u_1,\dots,u_m)$ and $V = (u_1,\dots,u_m)$ be two families of
vectors in $E,$ such that $u$ is linearly independent, $v = Au,$ and $A$
is a square matrix with a nonzero trace.
Suppose, with the goal of showing that $\lambda_i = 0\, \forall i,$ that 
$\displaystyle{\sum_{i=1}^\infty \lambda_i v_i = 0}.$ This is equivalent to
\[ \sum_{i=1}^m\lambda_i\left( \sum_{j=1}^m a_{i,j}u_j \right) = 
   \sum_{j=1}^m u_j\left( \sum_{i=1}^m \lambda_i a_{i,j} \right) = 0.
\]
Since $U$ is linearly independent, this implies that the inner sum is zero for
each $j.$ That is,
$\displaystyle{ \sum_{i=1}^m \lambda_i a_{i,j} = 0, \,\forall j }$
Since $a_{i,j} = 0$ for all $i > j,$ all terms with $j < i$ are zero. We can
rewrite this sum as
$\displaystyle{ \sum_{i=1}^j \lambda_i a_{i,j} = 0, \,\forall j }$

We can now show by induction on $j$ that $\lambda_j = 0$.
In the base case, $j = 1,$ so the sum evaluates to $\lambda_1 \cdot a_{1,1}$.
Since $a_{k,k} \neq 0\, \forall k$ by assumption, $\lambda_1 = 0$.

For the inductive case, we assume that $\lambda_k = 0\, \forall k < j.$ Thus
the sum evaluates to $\lambda_j a_{j,j} = 0$ which implies that $\lambda_j=0$.
We conclude that $\lambda_j = 0$ for all $1 \leq j \leq m,$ which implies that
$U$ is linearly independent, as desired.

\subsubsection{(2)}
$A$ is an $m\times m$ upper-triangular matrix with entries $a_{i,j},$ and the proof proceeds
by induction on $m$.

The base case is trivial, since a square matrix with one nonzero entry is, by
definition, upper-triangular. The inverse is $(a_{1,1}^{-1}),$ which clearly
gives $AA^{-1} = I.$

For the inductive case, suppose that 
\[A=\left(
\begin{array}{ccc|c}
& && \\
&B&&v\\
& && \\
\hline
&0&&r
\end{array}
\right)\]
where $B$ is some upper-triangular matrix, $v$ is a column vector with $m-1$
elements, and $r$ is nonzero. The induction hypothesis tells us that $B^{-1}$
exists and is upper-triangular. We construct the matrix $A^{-1} = (z_{i,j})$
as follows:
When $i,j < m)$, we assign $z_{i,j} = B^{-1}.$ We set $z_{m,j}=0$ for $j < m$,
and $z_{m,m} = r^{-1}.$ The last bit to work out is the column vector that
occupies the same region as $v.$
We now have
\[A^{-1}=\left(
\begin{array}{ccc|c}
& && \\
&B^{-1}&&u\\
& && \\
\hline
&0&&r^{-1}
\end{array}
\right)\]
Consider the product of the $(n-1)$th row of $A$ with the last column of $A^{-1}$. We require this to be 0, so 
\[A_{(n-1)(n-1)}u_{n-1} + A_{(n-1)n}r^{-1} = 0\]
By assumption the diagonal term of $A$ is nonzero, so there is a unique value of $u_{n-1}$ satisfying this equation. 
Now inductively, suppose $u_{n-1}\cdots u_k$ have been uniquely determined, and $k > 1$. Then from multiplying the $(k-1)$th row of $A$ with the last column of $A^{-1}$, we get
\[A_{(k-1)n}r^{-1}+\sum_{i=n-1}^{k-1} A_{(k-1)i} u_i=0\]
By assumption only $u_{k-1}$ can vary, and since $A_{(k-1)(k-1)}\neq 0$ by hypothesis, then $u_{k-1}$ is uniquely determined. 
\\Thus we can construct $\vec{u}$ this way to form $A^{-1}$ so that it behaves properly. 
%Now, we check to see that we actually have the correct inverse.
%\[z_{i,j} = \sum_{k=1}^m a_{i,k}z_{k,j} \]
%We need $z_{i,j} = 1$ when $i \neq j$ and $z_{i,j} = 0$ otherwise.
\\\\Now suppose our upper triangular matrix has zero diagonal entries. Let $n$ be the first column with a zero on the diagonal. Then the first $n$ column vectors span $n-1$ dimensions, so they are linearly dependent. This means that the matrix has linearly dependent column vectors; therefore it is not invertible. This proves the reverse direction. 


\subsubsection{(3)}
This follows as a direct consequence of (1) and (2). We only need to show that
$V$ being linearly independent implies that $U$ is linearly independent, since
(1) covers the converse. And if $U$ is linearly independent, then we can write
$V$ as linear combinations of elements of $U$. This is because we assume that
$V = AU,$ and knowing that $A$ is invertible lets us rewrite this as 
$A^{-1}V = U$.
Then we can apply (1) again to find that $U$ is linearly independent.

\subsection{Problem B3}
\subsubsection{(1)}
Each linear equation gives us $x_k + 2x_{k+1} = b_k$ when $k < n,$ and
$x_n = b_n$ from the case $k = n.$ We can solve the first equation to give us
$x_k = b_k - 2x_{k+1},$ which has the nice summation-form solution of
\[x_k  = \sum_{i=k}^nb_{i}(-2)^{i-k}\]

\subsubsection{(2)}
We already proved this in Problem B2(2)!
Actually, the inverse is quite nice. Each diagonal (\emph{i.e.} the set
$\{a_{i,j}|j-i=k\}$) has the same value, with value $(-2)^k$ ($k$ is the vertical
displacement from the diagonal).
Of course when $n=300$, the upper right-hand corner would be equal to
$-2^{299},$ which is too large to compute, since the universe doesn't have
enough atoms!

\subsubsection{(3)}
We can distribute the matrix $A$ through the sum since it is a linear operator. So
\[A(x+\delta x)=Ax+A\delta x = b+\delta b\]
We have a solution to $Ax=b$ from above. We can also solve $A\delta x=\delta b$ analogously, by
\[\delta x_k = \sum_{i=k}^n \delta b_i (-2)^{i-k} \]
Then the answer follows by taking $k=1$:
\[\delta x_1 = \sum_{i=1}^n \delta b_i (-2)^{i-1} = \epsilon (-2)^{n-1}\]
since for $1\le i < n, \delta b_i = 0$, and $\delta b_n = \epsilon$


\subsubsection{(4)}
We will show by induction on $m$ that $(A-I)^m$ has $m$ diagonals that are equal
to 0. That is, if $(A-I)^m = (z_{i,j}),$ then $z_{i,j} = 0$ when $i - j < m$,
and when $m \geq n$ the result is just 0.
The base case is trivially satisfied, since $(A-I)$ has zeros down the main
diagonal, and it's lower-triangular.

The inductive case assumes that $(z_{i,j}) = (A-I)^m$ satisfies 
$z_{i,j} = 0$ when $i - j < m$.
We consider $z'_{i,j} = (A-I)^m(A-I).$
By the definition of matrix multiplication, 
\[z'_{i,j} = \sum_{k=0}^m ((A-I)^m)_{i,k}(A-I)_{k,j}\]
Now suppose that $i - j < m + 1.$
Notice that $((A - I)^m)_{i,k} = 0$ for $i - k < m$, which can be rewritten as
$i - m < k$
Similarly, $(A - I)_{k,j} = 0$ for $k < j + 1$
Thus one of the two terms of the summation is zero when for $i - m < k$ and
for $k < j + 1$, which is the same as $i - m < j + 1$, or $i - j < m + 1$.
So $z_{i,j}$ is the sum of $n$ zeros when $i - m < j + 1$, as desired.

\subsection{Problem B4}
\subsubsection{(1)}
$B^2_0(t) = (1-t)^2 = 1 - 2t + t^2 = (1,-2,1)\cdot(1, t, t^2)$\\
$B^2_1(t) = 2(1-t)t = 2t - 2t^2 = (0,2,-2)\cdot(1, t, t^2)$\\
$B^2_1(t) = t^2 = (0,0,1) \cdot(1,t,t^2)$\\
And $B^2_0(t) + B^2_1(t) + B^2_2(t) = (1 - 2t + t^2) + (2t - 2t^2) + t^2 = 1.$

\subsubsection{(2)}
$B^3_0(t) = (1-t)^3 = 1 - 3t + 3t^2 - t^3 = (1,-3,3,-1)\cdot(1, t, t^2,t^3)$\\
$B^3_1(t) = 3(1-t)^2t = 3t - 6t^2 + 3t^3 = (0,3,-6,3)\cdot(1, t, t^2,t^3)$\\
$B^3_2(t) = 3(1-t)t^2 = 3t^2 -3t^3 = (0,0,3,-3)\cdot(1, t, t^2,t^3)$\\
$B^3_3(t) = t^3 = t^3 = (0,0,0,1)\cdot(1, t, t^2,t^3)$\\
And $B^3_0(t) + B^3_1(t) + B^3_2(t) + B^3_3(t) = (1 - 3t + 3t^2 - t^3) + (3t -
6t^2 + 3t^3) + (3t^2 -3t^3) + (t^3) = 1$

\subsubsection{(3)}
Question B4(2) shows the preconditions needed to apply B2(1), once we show that
$(1,t,t^2)$ and $(1,t,t^2,t^3)$ are each linearly independent. This follows
because they each clearly span the polynomials of respective maximum degree 2
and 3. Since these families are the minimal spanning families, they must be
bases and therefore linearly independent.

\subsubsection{(4)}
This follows from the Binomial Identity, which states that
\[(x + y)^n = \sum_{k=0}^n \binom{n}{m}x^{n-k}y^k\]
Then 
\[(1-t)^{n-k} = \sum_{j=0}^{n-k}\binom{n-k}{j}1^{n-k-j}(-1)^jx^j =
\sum_{j=0}^{n-k}\binom{n-k}{j}1^{n-k-j}(-1)^jx^j \]
Multiplying by $t^k$ gives
$(1-t)^{n-k}t^k = \displaystyle{\sum_{j=0}^{n-k}
\binom{n-k}{j}(-1)^jx^{j+k}}$
Then we can shift indices by $k$, replacing $j$ with $j-k$ to yield
\[(1-t)^{n-k}t^k = \sum_{j=k}^{n} \binom{n-k}{j-k}(-1)^{j-k}x^j\]
Then, we can multiply by $\binom{n}{k}$ to obtain
\[\binom{n}{k}(1-t)^{n-k}t^k = \sum_{j=k}^{n} \binom{n}{k}\binom{n-k}{j-k}(-1)^{j-k}x^j\]
Now, we just need to apply the identity $\binom{n}{k}\binom{n-k}{j-k} =
\binom{n}{j}\binom{j}{k}$ to see that 
\[\binom{n}{k}(1-t)^{n-k}t^k = \sum_{j=k}^{n} \binom{n}{j}\binom{j}{k}(-1)^{j-k}x^j\]

A proof of this last identity is as follows:
\[\binom{n}{k}\binom{n-k}{j-k} = \frac{n!(n-k)!}{k!(n-k)!(j-k)!(n-j)!}
=\frac{n!}{j!(n-j)!}\frac{j!}{k!(j-k)!} = \binom{n}{j}\binom{j}{k}\]

\medskip
As per part (2), we can write the column vectors
$B = \displaystyle{(B_0^m, B_1^m,\dots,B_m^m)}$ and $X = (1,t,\dots,t^m)$,
and the $m\times m$ matrix
$A = (a_{y,x})$ such that $a_{y,x} = (-1)^{x - y} \binom{m}{x}\binom{x}{y}t^x$.
This is defined so that $a_{y,x}$ is the $t^x$ term of $B^m_y$.
Note that this is consistent with the summation from part (4), since 
$\binom{x}{y} = 0$ and therefore the coefficient is zero when $y > x$. 
$A$ is upper-triangular with a non-zero trace, since the terms with $y=x$
evaluate to $a_{y,x} = \binom{m}{x} > 0$.
Thus by B2(2), $B = AX$ is linearly independent, since $X$ is linearly
independent and $A$ is upper-triangular with no zero entries on the diagonal.

\subsubsection{(5)}
We note that it suffices to show that the sum of the coefficients in each column
of the matrix $A$ from the previous section, with the exception of the first
column, is zero.
Then we just need to show that, for $x > 0,$
\[\sum_{y=0}^m (-1)^{x-y} \binom{m}{x}\binom{x}{y} = 0\]
Pulling out the unchanging first term, which can then be discarded since
$1 \leq x \leq m$, our goal is now
\[\sum_{y=0}^m (-1)^{x-y}\binom{x}{y} = 0\]
Now, since $\binom{x}{y} = 0$ when $y > x$, we can change the summation's bounds
to
\[\sum_{y=0}^x (-1)^{x-y}\binom{x}{y} = 0\]
We can also divide by $(-1)^x,$ which is also constant, making our goal
\[\sum_{y=0}^x(-1)^y \binom{x}{y} = 0\]
But this is the expansion of $(1 - 1)^x$ according to the Binomial Identity,
and this expression is equal to $0$, as desired.

\subsubsection{Extra Credit}
Incidentally, we answered each part of the extra credit along the way!

\subsubsection{(6)}
The first identity, $\displaystyle{\sum_{j=0}^{m-i}t^iB_j^{m-i}(t)} = t^i$ follows
directly from part (5). We merely multiply start with the identity from (5),
\[ \sum_{j=0}^{m-i}B^{m-i}_j(t) = 1 \textrm{, so }
\sum_{j=0}^{m-i}t^iB^{m-i}_j(t)=t^i \]
We have already proven the second identity in part (4), in the form 
\[\binom{n}{k}\binom{n-k}{j'-k} = \binom{n}{j'}\binom{j'}{k}\]
Just set $n = m, k = i$, and $j' = i + j$.
\medskip
By application of the first identity, the last identity is equivalent to
\[\sum_{j=0}^{m-i}t^iB_j^{m-i}(t) =
\sum_{j=0}^{m-i}\frac{\binom{i+j}{m}{i}}{\binom{m}{i}}B_{i+j}^m(t)\]
Since the bounds are equal, we just need to show that the summands are equal.
Applying the second identity means that our goal is, for appropriate $i,j,m,$
\[t^iB_j^{m-i}(t) = \frac{\binom{m-i}{j}}{\binom{m}{j+1}}B_{i+j}^m(t)\]
Then we only need to apply the definiton of Bernstein polynomials.
\[\binom{m-i}{j}(1-t)^{m-j}t^{i+j} =
\frac{\binom{m-i}{j}}{\binom{m}{i+j}}\binom{m}{i+j}(1-t)^{m-j}t^{i+j}\]
which proves the claim.

\medskip
We can then say that the $m + 1$ Bernstein polynomials with superscript $m$ span
a $m+1$-dimensonal vector space (with the alternative ``standard''
basis of $(1,t,\dots,t^m)$, which qualifies the Bernstein polynomials as a
minimal spanning family and therefore a basis.

\subsubsection{(7)}

\subsection{Problem B5}
\subsubsection{(1)}
Since $A$ is invertible, we know that $A^{1}$ exists.
We then assume that $AX = 0.$ We can left-multiply by $A^{-1},$ giving us
$A^{-1}AX =  A^{-1}0$, or $X = 0,$ as desired.

\subsubsection{(2)}
Suppose $I_m - AB$ is invertible. We have the identity
\\$B(I_m-AB)=B-BAB=(I_n-BA)B$ 
\\Multypling on the right by $(I_m-AB)^{-1}$ we get
\\$B=(I_n-BA)B(I_m-AB)^{-1}$ 
\\Now we multiply both sides by A on the right, and then add $I_n-BA$, to obtain
\\$I_n=(I_n-BA)B(I_m-AB)^{-1}A+I_n-BA$
\\Factoring the right side, we get
\\$I_n=(I_n-BA)*(B(I_m-AB)^{-1}+I_n)$
\\Thus $I_n-BA$ is invertible, as its inverse is given by $(B(I_m-AB)^{-1}+I_n)$.
%\\Similarly, we can write $(I_m - AB)^{-1} = (A(I_


\end{document}
