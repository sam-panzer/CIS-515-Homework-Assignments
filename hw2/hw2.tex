\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath}

\setcounter{secnumdepth}{0}
\newcommand{\emptyspace}{\{0\}}
\begin{document}

\begin{center}CIS 515 --- HW2\\Sam Panzer and Kevin Shi\end{center}
\subsection{Problem B1}
We'll do this by induction on $n$.
The base case is trivial, since then $AB := A_1(B_1^\top)^\top$.

For the inductive step, we assume that $n >= 1$, and we let $A'$ be the first
$n - 1$ columns of $A$ and $B'$ be the first $n - 1$ rows of $B$. Furthermore,
let $a$ be the last column of $A$ and $b$ be the last column of $B$.
Our induction hypothesis is that
$A'B' = A_1(B_1'^\top)^\top + \dots + A_{n-1}(B_{n-1}'^\top)^\top $.
Note that
\[(AB)_{i,j} = \left( \sum_{k=1}^{n-1} A_{i,k}B_{k,j} \right) + a_ib_j
             = (A'B')_{i,j} + a_ib_j\]
Because the sum is just $A'B'$, we can break this up to be
\[AB = A'B' + ab = A_1(B_1'^\top)^\top + \dots + A_{n-1}(B_{n-1}'^\top)^\top  +
ab\]
Since $ab$ is defined to be $A_n(B_n^\top)^\top$, we just cite the induction
hypothesis to conclude the proof.

\subsection{Problem B2}
$f: E \rightarrow F$ is a bijective lineasr map. To show that $f^{-1}$ is
linear, note that $f^{-1} \circ f = id$ and consider
\begin{eqnarray*}
f^{-1}(a) + f^{-1}(b) &=& id(f^{-1}(a) + f^{-1}(b) )\\
&=& f^{-1}(f(f^{-1}(a) + f^{-1}(b))) \\
&=&f^{-1}(f(f^{-1}(a)) + f(f^{-1}(b)))\\
&=& f^{-1}(a + b)
\end{eqnarray*}
\subsection{Problem B3}
\subsubsection{(1)}
This proof requires two directions.
First, let's assume that $z = (u_1,\dots,u_{i-1},v,u_{i+1}, \dots, u_p) \in Z_i$
for some $i$.
This means that 
\[a(z) = \sum_{j = 1, j \neq i}^p u_j + v 
      = \sum_{j = 1, j \neq i}^p u_j - \sum_{j = 1, j \neq i}^p u_j = 0\]
Thus $a(z) = 0$ for each $z$ in any $Z_i$. $a$'s linearity therefore implies
that any linear combination of vectors drawn from the $Z_i$ is therefore in its
kernel, which means that $\ker(a) \supseteq Z_1 + \dots + Z_p$.

For the other direction, we assume that $u = (u_1,\dots,u_p) \in \ker(a)$.
Then $u_i = -\sum_{j = 1, j \neq i} u_j$ for each $i$, since $a(u) = 0$ implies
that $u_1 + \dots + u_p = 0$.
This means that $u$ satisfies the criterion for membership in any $Z_i$, so it
must be a member of $Z_ + \dots + Z_p$.
Therefore $Z_ + \dots + Z_p \supseteq \ker(a)$, meaning that the two spaces are
equivalent.

\subsubsection{(2)}
The sum is a direct sum iff $a$ is injective.
First, let's assume that $a$ is in fact injective, which means that, for
$u_i,v_i \in U_i$,
$u_1 + \dots + u_p = v_1 + \dots + v_p$ iff $u_i = p_i \forall i$.
We need to show that
\[V_i = U_i \cap \left( \sum_{j=1,j\neq i}^p U_j \right)\]
is the trivial vector space for each $i$. We suppose that $x \in V,$ which means that
$x = u_i \in U_i$ and $x = \displaystyle{\sum_{j=1,j \neq i}^p} u_j$ where
$u_j \in U_j$.
Then we have $0 = u_i - \displaystyle{\sum_{j=1,j \neq i}^p} u_j$, and we can
look at this equation as comparing the values of $a$ applied to to $(0,\dots,0)$
and $(u_1,\dots,u_p)$. Therefore $a$'s injectivity implies that $(u_1,\dots,u_p)
= 0$, so we conclude that $x = 0$, as desired.

\medskip
For the other direction, we assume that if $x \in U_i$ and $x =
\displaystyle{\sum_{j=1,j \neq i}^p} u_j$ then $x = 0$.
If we then let $a(u_1,\dots,u_p) = a(v_1,\dots,v_p)$ with $u_j \in U_j$, then we
have $u_1 + \dots + u_p = v_1 + \dots + v_p$. In fact, for any $i$, this can
become $(u_i - v_i) = \displaystyle{\sum_{j=1,j \neq i}^p} v_j - u_j$. But then
we expressed an element of $U_i$ as the sum of one element from each other $U_j$
(since $u_k, v_k \in U_k$ for all $k$, their difference is in $U_k$), which
allows us to apply our assumption that such an element is 0.
Thus $(u_1,\dots,u_p) - (v_1,\dots,v_p) = 0$, which implies that the two vectors
are equal; $a$ must therefore be injective, as desired.

\subsubsection{(3)}
Given $f_1 + \dots + f_p = id$, we can compose any $f_i$ on the left of this to
obtain, by linearity, $f_i \circ f_1 + \dots + f_i \circ f_p = f_i$, or
\[f_i^2 + \sum_{j=1,j \neq i}^p f_i \circ f_j = f_i, \forall\, i\]

If we now assume that $f_i \circ f_j = 0$ when $i \neq j$, we immediately have
that $f_i^2 = f_i, \forall\, i$, since each term in the summaitois just 0.

The other direction is a little trickier. Let's assume that $f_i^2  = f_i,
\forall\, i$
Starting with the summation from above, we have
\[\sum_{j=1,j \neq i}^p f_i \circ f_j = 0 \]

\subsection{Problem B4}
We show that every vector $v\in E$ can be represented as a sum of components in the kernel and in the image of $f$. 
\\We write 
\[v = v + fv - fv = fv + (v - fv)\]
\\$fv\in $ im $f$ by definition of image. Now 
\[f(v-fv)=fv-f\circ fv=fv-fv=0\]
\\so $(v-fv)\in \ker f$ as desired, and hence $v$ decomposes into a sum of components in the kernel and image of $f$. 
\\Now $f\circ fv=fv$ so $f$ is the identity operator on its image. So when we apply $f$ to a vector $v$, it fixes the component of $v$ in the image of $f$ and kills the other part. Hence $f$ is the projection operator onto its image.

\subsection{Problem B5}
Let $A$ be an $n\times n$ matrix with elements $a_{y,x}$, and $e_{y,x,}$ be the
elements of $E_{i,j;\beta}$.
Then 
\[P = AE_{i,j;\beta} = A(I + (I - E_{i,j;\beta})) = A + A(I - E_{i,j;\beta})\]
By definition of $E_{i,j;\beta}$, we know $M = I - E_{i,j;\beta}$ is entirely
zeros except for a $\beta$ at row $i$ and column $j$.
Therefore any column of $P$ aside from the $j^\textrm{th}$ must be all zeros,
since $p_{y,x} = \sum_{k=1}^n a_{y,k}m_{k,x}$ is zero whenever $x \neq j$.
Even when $x = j$, the summand is 0 unless $k = i$, which implies that $p_{y,j}
= \beta p_{y,i}$ for all $1 \leq y \leq n$.
So $AM$ is all zeros except for the $j^\textrm{th}$ column, which is $\beta$
times the $i^\textrm{th}$ column of $A$.
Thus $P = A + AM$ is $A$ with $\beta$ times the $i^\textrm{th}$ column added to
the $j^\textrm{th}$.

\subsection{Problem B6}
\subsubsection{(a)}
\[
E=\left( 
\begin{array}{cccc}
1 &0 &0 &0\\
-1&1 &0 &0\\
0 &-1&1 &0\\
0 &0 &-1&1\\
\end{array}
\right)
\]
This works, since all we needed to do was subtract each row from the following
row.
\subsubsection{(b)}
This product subtracts row 4 from row 3, then row 3 from row 2, then row 2 from
row 1, followed by subtracting row 4 from row 3 again, then row 3 is subtracted
from row 2, and finally row 4 is subtracted from row 3 \emph{again}.
In other words, it ends up tunring the Pascal matrix into the identity!
\subsubsection{(c)}
Very funny. It's the product above, which is 
\[
E=\left( 
\begin{array}{cccc}
1 &0 &0 &0\\
-1&1 &0 &0\\
1 &2 &1 &0\\
-1&3 &-3&1\\
\end{array}
\right)
\]

\subsubsection{(d)}
As we're trying to go from the first $n$ rows of Pascal's triangle to $n-1$ rows
of it, all we need to do is subtract row $i$ from row $i+1$ from bottom to top.
This takes the form of
$E_{2,1;-1}E_{3,2;-1}\dots E_{n,(n-1); -1}E_{(n+1),n; -1}$.

\medskip
To calculate the inverse of $Pa_n$, we just repeat the above operation
recursively, getting the top row and first column right on each iteration.

\subsection{Problem B7}
Let $f$ be a linear operator with rank 1, so dim(im $f$) = 1. Then set $u = $ im $f$. 
\\Let $e_1 \in E$ be the vector such that $fe_1 = u$. 
\\Now let $E^T$ denote the orthogonal subspace of $E$ to $e_1$, and let $e_2,e_3,\cdots,e_n$ be an orthogonal basis for this space. So $e_1,\cdots,e_n$ is an orthogonal basis for $E$.
\\Then for every $v \in E^T,fv=0$, and for every $v=\lambda e_1,fv=\lambda u$. 
\\So the matrix representation of $f$ with respect to this basis is given by $uv$ where $v$ is the row vector $(1,0,\cdots ,0)$
\\For the reverse implication, if the matrix representation of $f$ is $A=uv$ where $u$ is a nonzero column vector of dimension $m$ and $v$ is a nonzero row vector of dimension $n$, then every row vector $r_i = u_i v$ is a scalar multiple of every other row vector $r_j = u_j v$, namely $r_i = \dfrac{u_j}{u_i}$. Hence this matrix has only one linearly independent row vector, so it has rank 1. So rank $f = 1$. 


\end{document}
