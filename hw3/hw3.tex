\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}

\setcounter{secnumdepth}{0}
\newcommand{\emptyspace}{\{0\}}
\newcommand{\reals}{\mathbb{R}}
\begin{document}

\begin{center}CIS 515 --- HW3\\Sam Panzer and Kevin Shi\end{center}
\subsection{Problem B1}
\subsubsection{(1)}
Since we can subtract one column from the others without affecting the
determinant and then expand by minors along the bottom row, we can say
\[
\left|
\begin{array}{ccc}
  a_1 & b_1 & c_1 \\
  a_2 & b_2 & c_2 \\
  1 & 1 & 1
\end{array}
\right|
=
\left|
\begin{array}{ccc}
  a_1 & b_1 - a_1 & c_1 -a_1 \\
  a_2 & b_2  - a_2& c_2  - a_2\\
  1 & 0 & 0
\end{array}
\right|
=\left|
\begin{array}{cc}
  b_1 - a_1 & c_1 - a_1 \\
  b_2 - a_2 & c_2 - a_2 \\
\end{array}
\right|
\]
which is zero iff $(b_1 - a_1, b_2 - a_2)$ and $(c_1 - a_1, c_2 - a_2)$ are
linearly dependent.
\subsubsection{(2)}
Similarly, we can again subtract the first column from the other columns and
expand the bottom row by minors, giving
\[
\left|
\begin{array}{cccc}
  a_1 & b_1 & c_1 & d_1 \\
  a_2 & b_2 & c_2  & d_2\\
  a_3 & b_3 & c_3  & d_3\\
  1 & 1 & 1 & 1
\end{array}
\right|
=
\left|
\begin{array}{cccc}
  a_1 & b_1 - a_1 & c_1 - a_1 & d_1 - a_1\\
  a_2 & b_2 - a_2 & c_2 - a_2 & d_2 - a_2\\
  a_3 & b_3 - a_3 & c_3 - a_3 & d_3 - a_3\\
  1 & 0 & 0
\end{array}
\right|
=\left|
\begin{array}{ccc}
  b_1 - a_1 & c_1 - a_1 & d_1 - a_1 \\
  b_2 - a_2 & c_2 - a_2 & d_2 - a_2\\
  b_3 - a_3 & c_3 - a_3 & d_3 - a_3\\
\end{array}
\right|
\]
which, again, is zero iff 
$(b_1 - a_1, b_2 - a_2, b_3 - a_3),(c_1 - a_1, c_2 - a_2, c_3 - a_3)$, and
$ (d_1 - a_1, d_2 - a_2, d_3 - a_3)$ are linearly dependent.
linearly dependent.

\subsection{Problem B2}
Let $A$ be positive definite, so for all vectors $v$ we have $v^TAv\ge 0$. For every vector $w$, let $v=Bw$, and $(Bv)^T=v^TB^T$, so $w^TB^TABw\ge 0$. Hence $B^TAB$ is positive definite.
\\Conversely, let $B^TAB$ be positive definite, so $v^TB^TABv \ge 0$ for every vector $v$. Since $B$ is invertible, then $B$ and $B^{-1}$ act transitively on the vector space; i.e. let $w$ be any vector and set $Bv=w$, so $v=B^{-1}w$. Then $(B^{-1}w)^TB^TABB^{-1}w\ge 0$. The expression on the left reduces to $w^TB^{-1^T}B^TAw$
\\Now we show $B^{-1^T}=B^{T^{-1}}$, i.e. transpose commutes with inverse. Start with the identity $B^{-1}B=I$. Now transpose both sides to get $B^TB^{-1^T}=I$. Take the inverse of both sides to get $B^{-1^{T^{-1}}}B^{T^{-1}}=I$. Finally multiply both sides by $B^{-1^T}$ to get the desired identity. 
Hence everything reduces to $w^TAw\ge 0$, and since $w$ is arbitrary this shows that $A$ is positive definite. 

\subsection{Problem B3}
\subsubsection{(1)}
If we do the following four row operations, we can transform $A$ into the
desired form. We can then recover $S$ by applying the four elementary matrices
to the identity matrix. For now, let's assume that $a$ is nonzero, though we'll
give a separate case for it later.

We divide the top row by a, then subtract $c$ times the first row from the
second row. Next, we zero out the upper-right corner by subtracting
$\dfrac{b}{ad-bc}$ times the bottom row from the top row. Finally, we multiply
the bottom row by $a$.

So our four elementary matrices are,
\[S_1 = E_{1,;a^{-1}}, S_2 = E_{2,1;-c}, S_3 = E_{1,2 ; \frac{-b}{ad-bc}};
S_4 = E_{2; a}\]
with $S = S_4 S_3 S_2 S_1$

When $a = 0$, we know $c \neq 0$ for $A$ to be invertible. So we first add
$c^{-1}$ times the second row to the first row for the first row operation; we
subtract $c$ times the first row from the second row to zero out the lower
left-hand corner; and finally we zero out the upper right-hand corner by
subtracting enough copies of the bottom row.

Thus there is always some invertible $S$ (which, incidentally, has determinant
1) that can be expressed as the product of three or four elementary matrices,
such that $SA$ is in the required form.

When $\det(A) = 1$, we have $SA = I$, which tells us that $S = A^{-1}$.
Thus $A$ is the product of at most four elementary matrices, namely the inverses
of $S_1$ through $S^4$.

\subsubsection{(2)}
Let $u = \dfrac{\cos(\theta) - 1}{\sin(\theta)}$ and $v = \sin(\theta)$.
Then 
\[
ULU = 
\left(
\begin{array}{cc}
  1 & u \\
  0 & 1
\end{array}
\right)
\left(
\begin{array}{cc}
  1 & 0 \\
  v & 1
\end{array}
\right)
\left(
\begin{array}{cc}
  1 & u \\
  0 & 1
\end{array}
\right)
= 
\left(
\begin{array}{cc}
  1 + uv & 2u + u^2v \\
  v & 1 + uv
\end{array}
\right)
=
\left(
\begin{array}{cc}
  \cos(\theta) & -\sin(\theta) \\
  \sin(\theta) & \cos(\theta)
\end{array}
\right)
\]

\subsubsection{(3)}
This is the typical programmer's trick for swapping two variables without using
a temporary variable.\\
\verb|y -= x; x += y; y -= x; y *= -1|

Let $E_1 = E_{i,j;-1}$, and $E_2 = E_{j,i,1}$.
Then $B = E_{j,-1} E_1 E_2 E_1$ does the following when we right-multiply by
$A$:
Subtracts row $i$ from row $j$, then adds row $j$ to row $i$, then subtracts row
$i$ from row $j$ again, and finally multiplies row $j$ by $-1$.
If we treat this as a single static assignment problem where $x_0$ holds row $i$
and $y_0$ holds row $j$, it's written as
\begin{eqnarray*}
y_1 &=& y_0 - x_0\\
x_1 &=& x_0 + y_1\\
y_2 &=& y_1 - x_1\\
y_3 &=& -y_2
\end{eqnarray*}
If we unfold the definitions, we find that $y_3 = x_0$ and $x_1 = y_0$.
Thus we successfully swap rows $i$ and $j$.

\medskip

\subsubsection{(4)}

\subsection{Problem 6}
\subsubsection{(1)} Let $R$ be the matrix with $r_{i,i}=1$ for all $1\le i\le n, r_{i,i+1}=2$ for $i\neq 1$. It is easy to verify that $RR^T=A$. 
\subsubsection{(2)} $\det(R)=1$ since $R$ is upper triangular, and $\det(R^T)=\det(R)=1$, so $\det(A)=\det(R)\det(R^T)=1$
\subsubsection{(3)} We do this by induction. The base case, $n=1$, is trivial. Now suppose the formula holds for all $n$ up to and including $N-1$. We shall compute the determinant of the $N$ dimensional matrix by cofactor expansion on the $N$th row starting from the entry $a_{NN}$. 
\\$a_{N,N}=5-\lambda,a_{N,N-1}=2$, and the rest of the $N$th row is 0 and does not contribute to the determinant. The cofactor matrix of $a_{N,N}$ is the dimension $N-1$ matrix without the last row and column, and by the inductive hypothesis this has determinant $p_{N-1}$. If we expand on $a_{N,N-1}$ and then on $a_{N-1,N}$, we get a dimension $N-2$ matrix formed by removing the last two rows and columns, and by inductive hypothesis this have determinant $p_{N-2}$. If we expand on any element of the $N-2$ row, the determinant is zero since the last column of that matrix is now 0.

\end{document}
