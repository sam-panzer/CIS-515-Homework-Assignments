\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}

\setcounter{secnumdepth}{0}
\newcommand{\emptyspace}{\{0\}}
\newcommand{\reals}{\mathbb{R}}
\begin{document}

\begin{center}CIS 515 --- HW3\\Sam Panzer and Kevin Shi\end{center}
\subsection{Problem B1}
\subsubsection{(1)}
Since we can subtract one column from the others without affecting the
determinant and then expand by minors along the bottom row, we can say
\[
\left|
\begin{array}{ccc}
  a_1 & b_1 & c_1 \\
  a_2 & b_2 & c_2 \\
  1 & 1 & 1
\end{array}
\right|
=
\left|
\begin{array}{ccc}
  a_1 & b_1 - a_1 & c_1 -a_1 \\
  a_2 & b_2  - a_2& c_2  - a_2\\
  1 & 0 & 0
\end{array}
\right|
=\left|
\begin{array}{cc}
  b_1 - a_1 & c_1 - a_1 \\
  b_2 - a_2 & c_2 - a_2 \\
\end{array}
\right|
\]
which is zero iff $(b_1 - a_1, b_2 - a_2)$ and $(c_1 - a_1, c_2 - a_2)$ are
linearly dependent.
\subsubsection{(2)}
Similarly, we can again subtract the first column from the other columns and
expand the bottom row by minors, giving
\[
\left|
\begin{array}{cccc}
  a_1 & b_1 & c_1 & d_1 \\
  a_2 & b_2 & c_2  & d_2\\
  a_3 & b_3 & c_3  & d_3\\
  1 & 1 & 1 & 1
\end{array}
\right|
=
\left|
\begin{array}{cccc}
  a_1 & b_1 - a_1 & c_1 - a_1 & d_1 - a_1\\
  a_2 & b_2 - a_2 & c_2 - a_2 & d_2 - a_2\\
  a_3 & b_3 - a_3 & c_3 - a_3 & d_3 - a_3\\
  1 & 0 & 0
\end{array}
\right|
=\left|
\begin{array}{ccc}
  b_1 - a_1 & c_1 - a_1 & d_1 - a_1 \\
  b_2 - a_2 & c_2 - a_2 & d_2 - a_2\\
  b_3 - a_3 & c_3 - a_3 & d_3 - a_3\\
\end{array}
\right|
\]
which, again, is zero iff 
$(b_1 - a_1, b_2 - a_2, b_3 - a_3),(c_1 - a_1, c_2 - a_2, c_3 - a_3)$, and
$ (d_1 - a_1, d_2 - a_2, d_3 - a_3)$ are linearly dependent.
linearly dependent.

\subsection{Problem B2}
Let $A$ be positive definite, so for all vectors $v$ we have $v^TAv\ge 0$. For every vector $w$, let $v=Bw$, and $(Bv)^T=v^TB^T$, so $w^TB^TABw\ge 0$. Hence $B^TAB$ is positive definite.
\\Conversely, let $B^TAB$ be positive definite, so $v^TB^TABv \ge 0$ for every vector $v$. Since $B$ is invertible, then $B$ and $B^{-1}$ act transitively on the vector space; i.e. let $w$ be any vector and set $Bv=w$, so $v=B^{-1}w$. Then $(B^{-1}w)^TB^TABB^{-1}w\ge 0$. The expression on the left reduces to $w^TB^{-1^T}B^TAw$
\\Now we show $B^{-1^T}=B^{T^{-1}}$, i.e. transpose commutes with inverse. Start with the identity $B^{-1}B=I$. Now transpose both sides to get $B^TB^{-1^T}=I$. Take the inverse of both sides to get $B^{-1^{T^{-1}}}B^{T^{-1}}=I$. Finally multiply both sides by $B^{-1^T}$ to get the desired identity. 
Hence everything reduces to $w^TAw\ge 0$, and since $w$ is arbitrary this shows that $A$ is positive definite. 

\subsection{Problem B3}
\subsubsection{(1)}
If we do the following four row operations, we can transform $A$ into the
desired form. We can then recover $S$ by applying the four elementary matrices
to the identity matrix. For now, let's assume that $a$ is nonzero, though we'll
give a separate case for it later.

We divide the top row by a, then subtract $c$ times the first row from the
second row. Next, we zero out the upper-right corner by subtracting
$\dfrac{b}{ad-bc}$ times the bottom row from the top row. Finally, we multiply
the bottom row by $a$.

So our four elementary matrices are,
\[S_1 = E_{1,;a^{-1}}, S_2 = E_{2,1;-c}, S_3 = E_{1,2 ; \frac{-b}{ad-bc}};
S_4 = E_{2; a}\]
with $S = S_4 S_3 S_2 S_1$

When $a = 0$, we know $c \neq 0$ for $A$ to be invertible. So we first add
$c^{-1}$ times the second row to the first row for the first row operation; we
subtract $c$ times the first row from the second row to zero out the lower
left-hand corner; and finally we zero out the upper right-hand corner by
subtracting enough copies of the bottom row.

Thus there is always some invertible $S$ (which, incidentally, has determinant
1) that can be expressed as the product of three or four elementary matrices,
such that $SA$ is in the required form.

When $\det(A) = 1$, we have $SA = I$, which tells us that $S = A^{-1}$.
Thus $A$ is the product of at most four elementary matrices, namely the inverses
of $S_1$ through $S^4$.

\subsubsection{(2)}
Let $u = \dfrac{\cos(\theta) - 1}{\sin(\theta)}$ and $v = \sin(\theta)$.
Then 
\[
ULU = 
\left(
\begin{array}{cc}
  1 & u \\
  0 & 1
\end{array}
\right)
\left(
\begin{array}{cc}
  1 & 0 \\
  v & 1
\end{array}
\right)
\left(
\begin{array}{cc}
  1 & u \\
  0 & 1
\end{array}
\right)
= 
\left(
\begin{array}{cc}
  1 + uv & 2u + u^2v \\
  v & 1 + uv
\end{array}
\right)
=
\left(
\begin{array}{cc}
  \cos(\theta) & -\sin(\theta) \\
  \sin(\theta) & \cos(\theta)
\end{array}
\right)
\]

\subsubsection{(3)}
This is the typical programmer's trick for swapping two variables without using
a temporary variable.\\
\verb|y -= x;    x += y;    y -= x;    y *= -1|

Let $E_1 = E_{i,j;-1}$, and $E_2 = E_{j,i,1}$.
Then $B = E_{j,-1} E_1 E_2 E_1$ does the following when we right-multiply by
$A$:
Subtracts row $i$ from row $j$, then adds row $j$ to row $i$, then subtracts row
$i$ from row $j$ again, and finally multiplies row $j$ by $-1$.
If we initialize $x$ to hold row $i$ (let's call this value $x_0$)
and $y$ to holds row $j$ (called $y_0$), then ``execution'' of the statements
follows like this:
\begin{eqnarray*}
y -= x &;&  x = x_0, y = y_0 - x_0 \\
x += y &;& x = y_0, y = y_0 - x_0\\
y -= x &;& x = y_0, y = - x_0\\
y *= -1 &;& x = y_0, y = x_0\\
\end{eqnarray*}
At the end, $y$ and $x$ have swapped values.
Thus we successfully swap rows $i$ and $j$.

\medskip
This time, it's a little more complicated than a standard programming trick:\\
\verb|y *= d;    x += y/d;    y -= (d-1) * i;    x -= y;    y += (d-1)/d * i|
So, we let 
\[E_3 = E_{i,n,\frac{1}{d}}, E_4 = E_{n,i,-(d-1)}, E_5 = E_{i,n,-1},
  E_6 = E_{n,i, (d-1)/d} \]
As before, we consider $x$ to hold row $i$ (called $x_0$) and
$y$ to hold row $n$ (caled $y_0$). Then ``execution'' is as follows;
\begin{eqnarray*}
y *= d                &;& x = x_0, y = d\cdot y_0 \\
x += y/d              &;& x = x_0 + y_0, y = d\cdot y_0 \\
y -= (d-1)\cdot x     &;& x = x_0 + y_0, y = y_0 - (d-1) \cdot x_0\\
x -= y                &;& x = d \cdot x_0, y = y_0 - (d-1) \cdot x_0\\ 
y += -(d-1)/d \cdot x &;& x = d \cdot x_0, y = y_0
\end{eqnarray*}
We end with $y$'s original value restored and $x$ with $d$ times its
initial value. So $E_6E_5E_4E_3E_{n_d}$ multiplies row $i$ by $d$.

When $d = -1$, we find that $E_3 = E_5$ and $E_4 = E_6$.

Since we can write any transposition as the product of elementary matrices, and
we can write any permutation (matrix) as the product of (row) transpositions, we
can clearly write any permutation matrix as the product of the elementary
matrices that comprise the row transpositions.

\subsubsection{(4)}
We can see that such an $S$ exists by doing Gauss-Jordan elimination on $A$,
followed by using part (3) to multiply the bottom row by $d$. This works,
because Gauss-Jordan elimination consists only of elementary operations - no
permutations are needed because $A$ is invertible. (In the case that the
diagonal entry we're trying to turn into a 1 is a zero, we can just add one of
the rows below it that has a nonzero row in the desired column.)
Thus we only need elementary row operations to turn $A$ into $E_{n,d}$

\medskip
For $SL_n$, the above result yields $SA = I$, which implies that any
$A \in SL_n$ has an inverse that is expressible as the product of several
elementary matrices. We can apply this fact to the inverse of every matrix in
$SL_n$ to obtain it for the original matrix.

As for the operations, it takes up to $n$ operations to finish each column:
one to ensure that the diagonal element is a 1 (either through scaling or by
subtracting another row), and $n-1$ to ensure that all the other items in the
column are zeros. Across $n$ rows, this gives $n^2$ operations.

We're really not sure where $n(n+1) - 2$ comes from.

\subsection{Problem 4}
Firstly we know all diagonal entries are nonzero. Hence we can start with row 1 and make the first entry of row 2 zero by an elementary operation. Furthermore since $a_{11}>a_{21}$ then the constant we multiply by is less than 1, so after this operation $a_{22}> a_{22}^0 - |a_{12}|\ge a_{22}^0 - \displaystyle\sum_{i=1; i\neq 2}^n |a_{i2}|>0$, where the notation with a superscript 0 denotes the entry in the original matrix before we started operations, and where the last inequality comes from diagonal dominance. Hence we can perform the next elementary operation using row 2 without the need for pivoting.
\\Inductively, after $k-1$ operations, $a_{kk}>a_{kk}^0 - \displaystyle\sum_{i=1; i\neq k}^k |a_{ik}| > a_{kk}^0 - \displaystyle\sum_{i=1; i\neq k}^n |a_{ik}| > 0$. Hence we can perform the $k$th operation using the entry $a_{kk}$ without needing to pivot. Furthermore $a_{kk}>a_{(k+1)k}$ since $a_{kk}\ge \displaystyle\sum_{i=1; i\neq k}^n |a_{ik}|$ and the latter summation includes the term $a_{(k+1)k}$. Hence the scalar we multiply by to perform the $k$th row operation is less than 1, so afterwards $a_{(k+1)(k+1)}>a_{(k+1)(k+1)}^0 - \displaystyle\sum_{i=1; i\neq k+1}^{k+1} |a_{i(k+1)}>0$. 

\subsection{Problem 5}
\subsubsection{(1)}
This is a direct result of part (2).

\subsubsection{(2)}
\[L_i(x) = \prod_{j=1, j \neq i}^{m+1}\frac{x-\alpha_j}{\alpha_i-\alpha_j}\]
Consider $L_i(\alpha_k)$. If $i = k$, then we have 
\begin{eqnarray*}
L_i(\alpha_i) 
&=& \prod_{j=1, j \neq i}^{m+1}\frac{\alpha_i-\alpha_j}{\alpha_i-\alpha_j}\\
&=& \prod_{j=1, j \neq i}^{m+1}1 = 1
\end{eqnarray*}
If $i \neq k$, then 
\begin{eqnarray*}
L_i(\alpha_i) 
&=& \prod_{j=1, j \neq i}^{m+1}\frac{\alpha_k-\alpha_j}{\alpha_i-\alpha_j}\\
&=& \frac{\alpha_k-\alpha_k}{\alpha_i-\alpha_k}\prod_{j=1, j \neq i, j \neq
k}^{m+1}\frac{\alpha_k-\alpha_j}{\alpha_i-\alpha_j} \\
&=& 0
\end{eqnarray*}
Thus $L_i(\alpha_k) = \delta_{i,k}$.
\medskip

Now, we consider
\[P(x) = \sum_{i=1}^{m+1}
    \left(\beta_i \prod_{j=1, j \neq
    i}^{m+1}\frac{x-\alpha_j}{\alpha_i-\alpha_j}\right)
     = \sum_{i=1}^{m+1} \beta_i L_i(x)
    \]
and look at $P(\alpha_k)$
\begin{eqnarray*}
P(\alpha_k) &=& \sum_{i=1}^{m+1} \beta_i L_i(\alpha_k) \\
&=& \sum_{i=1}^{m+1} \beta_i \delta_{i,k}\\
&=& \beta_k
\end{eqnarray*}

\subsection{Problem 6}
\subsubsection{(1)} Let $R$ be the matrix with $r_{i,i}=1$ for all $1\le i\le n, r_{i,i+1}=2$ for $i\neq 1$, and 0 for all other entries. It is easy to verify that $RR^T=A$. 
\subsubsection{(2)} $\det(R)=1$ since $R$ is upper triangular with all 1's on the diagonal, and $\det(R^T)=\det(R)=1$, so $\det(A)=\det(R)\det(R^T)=1$
\subsubsection{(3)} We do this by induction. The base case, $n=1$, is trivial. Now suppose the formula holds for all $n$ up to and including $N-1$. We shall compute the determinant of the $N$ dimensional matrix by cofactor expansion on the $N$th row starting from the entry $a_{NN}$. 
\\$a_{N,N}=5-\lambda,a_{N,N-1}=2$, and the rest of the $N$th row is 0 and does not contribute to the determinant. The cofactor matrix of $a_{N,N}$ is the dimension $N-1$ matrix without the last row and column, and by the inductive hypothesis this has determinant $p_{N-1}$. If we expand on $a_{N,N-1}$ and then on $a_{N-1,N}=2$, we get a dimension $N-2$ matrix formed by removing the last two rows and columns, and by inductive hypothesis this have determinant $p_{N-2}$. If we expand on any element of the $N-2$ row, the determinant is zero since the last column of that matrix is now 0.
\\Hence the final formula for the determinant is $5p_{N-1}-4p_{N-2}$

\end{document}
