\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\setcounter{secnumdepth}{0}
\newcommand{\emptyspace}{\{0\}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\ints}{\mathbb{Z}}
\newcommand{\trace}{\textrm{tr}}
\newcommand{\simtwo}{\textbf{SIM(2)}}
\newcommand{\norm}[1]{||#1||}
\newcommand{\bignorm}[1]{\left|\left|#1\right|\right|}
\newcommand{\twomatrix}[4]{\left(\begin{array}{cc} #1 & #2\\ #3 & #4 \end{array}\right)}
\newcommand{\threematrix}[9]{\left(\begin{array}{ccc} #1 & #2 & #3\\ #4 & #5 & #6 \\ #7 & #8 & #9 \end{array}\right)}

\begin{document}

\begin{center}CIS 515 --- HW5\\Sam Panzer and Kevin Shi\end{center}
\subsection{B1}
Let $\alpha =\norm{A} \geq 0$, and note that $\norm{A^p} \leq \norm{A}^p$ by the definition of a matrix norm.
Then 
\begin{eqnarray*}
\bignorm{I + \sum_{k=1}^m \frac{A^k}{k!}} &\leq&
\norm{I} + \sum_{k=1}^m \frac{\norm{A^k}}{k!} \text{,  by triangle inequality }\\
&\leq& 1 + \sum_{k=1}^m \frac{\alpha^k}{k!} = \sum_{k=0}^m \frac{\alpha^k}{k!}\\
&\leq& \sum_{k=1}^\infty \frac{\alpha^k}{k!} = e^\alpha = e^{\norm{A}}
\end{eqnarray*}
Thus
\[ \bignorm{I + \sum_{k=1}^m \frac{A^k}{k!}} \leq e^\norm{A}\]
And since this sum is monotonic increasing and bounded above, it must have a limit.
Since the norms of the matrices in the sequence $E_m$ converge, the sequence itself must converge.
Therefore it is well-defined to denote the limit of this sequence by $e^A$.

\subsection{B2}
\subsubsection{(a)}
Note that these matrices are spanned by this linear combination:
\[
\theta \left(\begin{array}{ccc}
  0 & -1 & 0\\
  1 & 0 & 0 \\
  0 & 0 & 0 
\end{array}\right) 
+
u \left(\begin{array}{ccc}
  0 & 0 & 1\\
  0 & 0 & 0 \\
  0 & 0 & 0 
\end{array}\right) 
+
v \left(\begin{array}{ccc}
  0 & 0 & 0\\
  0 & 0 & 1 \\
  0 & 0 & 0 
\end{array}\right) 
\]
And the three component matrices are clearly linearly independent, because they do not share any nonzero entries.
%It's also clear that adding two such matrices, or multiplying them by an element of $\reals$ gives another vector in this set; the matrices $0$ and $I$ inherit their behavior from $M_3(\reals)$
Thus this is a subspace of $M_3(\reals)$ and isomorphic to $(\reals^3,+)$

Now consider $B = (\theta, u, v)$ and $C = (\theta', u', v')$.  Then 
\[
BC = 
\left(\begin{array}{ccc}
  -\theta\theta' & 0 & -\theta v'\\
  0 & -\theta \theta' & -\theta u' \\
  0 & 0 & 0 
\end{array}\right)  \text{ and }
CB = 
\left(\begin{array}{ccc}
  -\theta\theta' & 0 & -\theta' v\\
  0 & -\theta \theta' & -\theta' u \\
  0 & 0 & 0 
\end{array}\right)
\]
Note that the primes switched places, which shows that $BC$ is not necessarily equal to $CB$.

\subsubsection{(b)}
First, let $\theta = 0$. We then have that 
\[ B = 
\left(\begin{array}{ccc}
  0 & 0 & u \\
  0 & 0 & v \\
  0 & 0 & 0 
\end{array}\right) 
\]
Note that $B^2 = 0$. We immediately see that 
\[ e^B = I + \sum_{k=1}^\infty \frac{B^k}{k!} = 
         I + B + \sum_{k=2}^\infty \frac{B^k}{k!} =
         I + B + \sum_{k=2}^\infty \frac{0}{k!} = I + B
\]

For the second case ($\theta \neq 0$), we do a similar simplification.
\[
B^3 = B\cdot B^2 = B \cdot 
\left(\begin{array}{ccc}
  -\theta & 0 & -v\theta \\
  0 & -\theta & -u\theta \\
  0 & 0 & 0 
\end{array}\right) 
=
\left(\begin{array}{ccc}
  0 & -\theta ^2 & -u\theta^2 \\
  -\theta^2 & 0 & -v\theta^2 \\
  0 & 0 & 0 
\end{array}\right) 
= -\theta^2 B
\]
Then
\begin{eqnarray*}
  e^B &=&  I + B + \frac{B^2}{2!} + \frac{B^3}{3!} + \frac{B^4}{4!} + \frac{B^5}{5!} + \frac{B^6}{6!} + \dots \\
  &=&  I + B + \frac{B^2}{2!} + \frac{-\theta^2 B}{3!} + \frac{-\theta^2 B^2}{4!} + \frac{\theta^4 B}{5!} + \frac{\theta^4 B^2}{6!} + \dots \\
  &=&  I + \left(B - \frac{\theta^2 B}{3!} + \frac{\theta^4 B}{5!} - \dots \right) + 
           \left(\frac{B^2}{2!} - \frac{\theta^2 B}{4!} + \frac{\theta^4 B}{6!} - \dots \right)\\
  &=&  I + B \left(1 - \frac{\theta^2}{3!} + \frac{\theta^4}{5!} - \dots \right) + 
           B^2\left(\frac{1}{2!} - \frac{\theta^2}{4!} + \frac{\theta^4}{6!} - \dots \right)\\
\end{eqnarray*}
Note that $\sin(\theta) = \theta - \frac{\theta^3}{3!} + \frac{\theta^5}{5!} - \dots $,
which means that $\frac{\sin(\theta)}{\theta} = 1 - \frac{\theta^2}{3!} + \frac{\theta^5}{5!} - \dots $

\noindent
Similarly, $\cos(\theta) = 1 - \frac{\theta^2}{2!} + \frac{\theta^4}{4!} - \dots $, so
$\frac{1 - \cos(\theta)}{\theta^2} = \frac{1}{2!} - \frac{\theta^2}{4!} + \frac{\theta^4}{6!} - \dots $

\noindent
Thus we have
\[e^B = I + B\frac{\sin(\theta)}{\theta} + B^2 \frac{1 - \cos(\theta)}{\theta^2} \text{, or }\]
\[
  e^B = 
\left(\begin{array}{ccc}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 
\end{array}\right) 
+
\left(\begin{array}{ccc}
  0 & -\sin(\theta) & u\frac{\sin(\theta)}{\theta} \\
  -\sin(\theta) & 0 & v\frac{\sin(\theta)}{\theta} \\
  0 & 0 & 0 
\end{array}\right) 
+
\left(\begin{array}{ccc}
  \cos(\theta) - 1 & 0 & v\frac{\cos(\theta) - 1}{\theta} \\
  0 & \cos(\theta) - 1 & u\frac{\cos(\theta) - 1}{\theta} \\
  0 & 0 & 0 
\end{array}\right) 
\]
which gives the desired result.

\subsubsection{(c)}
The form of $e^B$ in the previous section confirms that $e^B$ is in \textbf{SE}$(2)$.
To show injectivity, we let $w_1,w_2 \in \reals$.
Then all we need to show is that, given $\theta$, we can find $u$ and $v$ such that
$u\frac{\sin(\theta)}{\theta} + v \frac{\cos(\theta) - 1}{\theta} = w_1$ and
$u\frac{1-\cos(\theta)}{\theta} + v \frac{\sin(\theta)}{\theta} = w_2$.
This is equivalent to solving the system
\[
\left(\begin{array}{cc}
\frac{\sin(\theta)}{\theta} & \frac{\cos(\theta) - 1}{\theta} \\
\frac{1-\cos(\theta)}{\theta} & \frac{\sin(\theta)}{\theta}
\end{array}\right)
\binom{u}{v} = \binom{w_1}{w_2} % Abuse of notation? sure!
\]
Which is of course solvable with exactly one solution when the determinant of the $2\times2$ matrix $T$ is nonzero.
Well, we end up with $\det(T) = -\frac{2\cos(\theta)}{\theta}$, which is nonzero (and defined) so long as $\theta \neq k\pi + \frac{\pi}{2}, k \in \ints$ (and we already assumed that $\theta \neq 0$).
So our map is injective so long as $\theta$ isn't a half-multiple of $\pi$.

\subsection{B3}
\subsubsection{(a)}
The group operation is composition, and the identity is the map where $\alpha = 1, \theta = w_1=w_2 =0,$.

Now, we just need to show that if $R\circ S \in$ \simtwo for any $R,S \in$ \simtwo.
Let $R$ have parameters $(\theta_1, w_1, z_1, \alpha_1)$ and $S$ have parameters $(\theta_1, w_1, z_1, \alpha_1)$. Then
\begin{eqnarray*}
  (R \circ S)\binom{x}{y} &=&  R(S\binom{x}{y}) \\
  &=& \alpha_2 
    \left(\begin{array}{cc}
      \cos(\theta_2) & -\sin(\theta_2)\\
      \sin(\theta_2) & \cos(\theta_2)
    \end{array}\right) 
  \left(\alpha_1 
    \left(\begin{array}{cc}
      \cos(\theta_1) & -\sin(\theta_1)\\
      \sin(\theta_1) & \cos(\theta_1)
    \end{array}\right)\binom{x}{y} + \binom{w_1}{z_1}\right) + \binom{w_2}{z_2}\\
&=& \alpha_1\alpha_2 \left(\begin{array}{cc}
      \cos(\theta_1)\cos(\theta_2) -\sin(\theta_1)\sin(\theta_2)& 
      -\sin(\theta_1)\cos(\theta_2) -\cos(\theta_1)\sin(\theta_2)\\
      \cos(\theta_1)\sin(\theta_2) +\sin(\theta_1)\cos(\theta_2)& 
      -\sin(\theta_1)\sin(\theta_2) +\cos(\theta_1)\cos(\theta_2)\\
    \end{array}\right)\binom{x}{y}\\
    &&+
  \left(\begin{array}{cc}
      \alpha_2\cos(\theta_2)w_1 -\alpha_2\sin(\theta_2)z_1 + w_2 \\
      \alpha_2\sin(\theta_2)w_1 +\alpha_2\sin(\theta_2)z_1 + z_2 \\
    \end{array}\right) \\
    &=& 
  \alpha_1\alpha_2\left(\begin{array}{cc}
      \cos(\theta_1 + \theta_2) & -\sin(\theta_1 + \theta_2)\\
      \sin(\theta_1 + \theta_2) & \cos(\theta_1 + \theta_2)
    \end{array}\right)\binom{x}{y} +
  \left(\begin{array}{cc}
      \alpha_2\cos(\theta_2)w_1 -\alpha_2\sin(\theta_2)z_1 + w_2 \\
      \alpha_2\sin(\theta_2)w_1 +\alpha_2\cos(\theta_2)z_1 + z_2 \\
    \end{array}\right) \\
\end{eqnarray*}
Which is also in \simtwo.
This formulation shows that for $R$ to be $S^{-1}$, we need $R$'s parameters to be
\begin{eqnarray*}
  \theta_2 &=&  -\theta_1\\
  w_1 &=& \alpha_1^{-1}(\sin(-\theta_1)z_1 - \cos(-\theta_1)w_1)\\
  z_1 &=& \alpha_1^{-1}(\cos(-\theta_1)z_1 + \cos(-\theta_1)w_1)\\
  \alpha_2 &=&  \alpha_1^{-1}
\end{eqnarray*}
Substituting this into the above equality gives $(R \circ S) \binom{x}{y} = \binom{x}{y}$, the identity element, so $R = S^{-1}$
\subsubsection{(b)}
We're looking at a subspace of $\reals^9$, such that there are only four distinct parameters.
We can therefore express any element of $\mathfrak{sim}(2)$ as
\[
\lambda \left(\begin{array}{ccc}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 0
    \end{array}\right) 
 + \theta \left(\begin{array}{ccc}
      0 & -1 & 0\\
      1 & 0 & 0\\
      0 & 0 & 0
    \end{array}\right) 
 + u\left(\begin{array}{ccc}
      0 & 0 & 1\\
      0 & 0 & 0\\
      0 & 0 & 0
    \end{array}\right) 
 + v\left(\begin{array}{ccc}
      0 & 0 & 0\\
      0 & 0 & 1\\
      0 & 0 & 0
    \end{array}\right) 
\]
This makes it clear that the four matrices span $\mathfrak{sim}(2)$; they are also clearly linearly independent.
We therefore have a simple bijection between $\mathfrak{sim}(2)$ and $\reals^4$.
Thus $\mathfrak{sim}(2)$ is isomorphic to $\reals^4$.
\subsubsection{(c)}
If we let follow the suggestion and let $\Omega$ and $J$ be defined as in the assignment,
we can verify that $J^2 = -I$ by high technology.
Now, we need to show this by induction on $k$:
\[ \Omega^k = \frac{1}{2}\left((\lambda + i\theta)^k + (\lambda + i\theta)^k \right)I
  + \frac{1}{2i}\left((\lambda + i\theta)^k - (\lambda + i\theta)^k \right) J\]
We start with a base case of $k=0$, using the convention that $A^0 = I$ for any square matrix $A$:
For the base case, $\Omega^0 = I$, so we just need to show that the expression evaluates to $I$.
\[
  \frac{1}{2}\left((\lambda + i\theta)^0 + (\lambda + i\theta)^0 \right)
  + \frac{1}{2i}\left((\lambda + i\theta)^0 - (\lambda + i\theta)^0 \right) = \frac{1}{2} (2I) + \frac{1}{2i}(0)J = I
\]

We now assume that that this equality holds for $k$, and try to show tha it is also true for $k+1$.
\begin{eqnarray*}
\Omega^{k+1} &=&  \Omega^k \cdot \Omega\\
&=& \left( \frac{1}{2}\left((\lambda + i\theta)^k + (\lambda + i\theta)^k \right)I
  + \frac{1}{2i}\left((\lambda + i\theta)^k - (\lambda + i\theta)^k \right)J \right)(\lambda I + \theta J)\\
  &=& \frac{1}{2}\lambda\left( (\lambda + i\theta)^k + (\lambda - i\theta)^k \right)I  
     - \frac{1}{2i}\theta\left( (\lambda - i\theta)^k - (\lambda - i\theta)^k \right)I +\\
  &&  \frac{1}{2i}\lambda\left( (\lambda + i\theta)^k + (\lambda - i\theta)^k \right)J +
      \frac{1}{2}\theta\left( (\lambda + i\theta)^k + (\lambda - i\theta)^k \right)J\\
  &=& \frac{1}{2}\left( \lambda(\lambda + i\theta) + \lambda(\lambda - i\theta)^k +
                        i\theta(\lambda + i\theta)^k - i\theta(\lambda-i\theta)\right)I + \\
  && \frac{1}{2i}\left( \lambda(\lambda + i\theta) - \lambda(\lambda - i\theta)^k +
                        i\theta(\lambda + i\theta)^k + i\theta(\lambda-i\theta)\right)J\\
  &=& \frac{1}{2}\left((\lambda + i\theta)(\lambda + i\theta)^k +
                       (\lambda - i\theta)(\lambda - i\theta)^k \right)I + \\
  && \frac{1}{2}\left((\lambda + i\theta)(\lambda + i\theta)^k -
                       (\lambda - i\theta)(\lambda - i\theta)^k \right)J \\
  &=& \frac{1}{2}\left((\lambda + i\theta)^{k+1} + (\lambda - i\theta)^{k+1} \right)I
  + \frac{1}{2i}\left((\lambda + i\theta)^{k+1} - (\lambda + i\theta)^{k+1} \right) J
\end{eqnarray*}
With that done, we can now look at
\begin{eqnarray*}
  e^\Omega &=&  \sum_{k=0}^\infty\frac{1}{k!}\Omega^k\\
  &=& \sum_{k=0}^\infty \frac{1}{k!}\left[ \frac{1}{2}\left((\lambda + i\theta)^k + (\lambda - i\theta)^k \right)I
    +\frac{1}{2i}\left((\lambda + i\theta)^k - (\lambda + i\theta)^k \right)J \right]\\
    &=& \frac{1}{2}I\left( \sum_{k=0}^\infty \frac{1}{k!}(\lambda + i\theta)^k +
            \sum_{k=0}^\infty \frac{1}{k!}(\lambda - i\theta)^k\right)
    +\frac{1}{2i}J\left(\sum_{k=0}^\infty \frac{1}{k!}(\lambda + i\theta)^k - 
    \sum_{k=1}^\infty \frac{1}{k!}(\lambda + i\theta)^k \right)\\
    &=& \frac{1}{2} \left( e^{\lambda + i\theta} + e^{\lambda - i\theta}  \right)I +
       \frac{1}{2i} \left( e^{\lambda + i\theta} - e^{\lambda - i\theta}  \right)J \\
    &=& e^\lambda(\cos(\theta)I + \sin(\theta)J)\\
    &=& e^\lambda\twomatrix{\cos(\theta)}{-\sin(\theta)}{\sin(\theta)}{\cos(\theta)}
\end{eqnarray*}
Whew!
\subsubsection{(d)}
We'll do this first proof by induction.
The base case is verified by $B^1 = \twomatrix{\Omega}{U}{0}{0}$. And the inductive case is
\[B^{n+1} = B^n\cdot B = \twomatrix{\Omega^n}{\Omega^{n-1} U}{0}{0} \twomatrix{\Omega}{U}{0}{0}
          = \twomatrix{\Omega^{n+1}}{ \Omega^n U}{0}{0}\]

Now, 
\begin{eqnarray*}
  e^B &=&  I + \sum_{k=1}^\infty \frac{1}{k!}\twomatrix{\Omega^k}{\Omega^{k-1}U}{0}{0}\\
  %&=& I + \sum_{k=1}^\infty \twomatrix{I +\frac{1}{k!}\Omega^k}{\frac{1}{k!}\Omega^{k-1}U}{0}{0}\\
  &=& \twomatrix{I + \sum_{k=1}^\infty \frac{1}{k!}\Omega^k}{\sum_{k=1}^\infty \frac{1}{k!}\Omega^{k-1}U}{0}{1}\\
\end{eqnarray*}
The upper-left entry is just the definition of $e^B$, and the upper-right entry is
\[
\sum_{k=1}^\infty \frac{1}{k!}\Omega^{k-1}U = I + \sum_{k=2}^\infty \frac{1}{k!}\Omega^{k-1}U 
= I + \sum_{k=1}^\infty \frac{1}{(k+1)!}\Omega^{k}U \text{ as desired }
\]

\subsubsection{(e)}
Let's start by unfolding the definition of the integral.
\begin{eqnarray*}
  \int_0^1 e^{\Omega t}dt &=& \int_0^1 Idt + \int_0^1 \left( \sum_{k=1}^\infty \frac{t^k\Omega^k}{k!} \right) dt\\
  &=& I + \sum_{k=1}^\infty\left( \int_0^1 \frac{t^k}{k!}\Omega^k \right)
  = I + \sum_{k=1}^\infty\left( \Omega^k \int_0^1 \frac{t^k}{k!}\right)\\
  &=&  I + \sum_{k=1}^\infty\left( \Omega^k \left[\frac{t^{k+1}}{(k+1)!}\right]_0^1\right)
  = I + \sum_{k=1}^\infty \frac{\Omega^k }{(k+1)!} = V
\end{eqnarray*}
This, of course, depends on the power series of a matrix being uniformly convergent, since we switch the integral and summation on the second line.
I will assume that power series of matrices are uniformly convergent just as power series of complex numbers are, and that the result I'm relying on (that we can switch these operations) goes through without a hitch.

Then, if $\theta = \lambda = 0$, then $\Omega = 0$, so $V = I + \sum_{k=1}^\infty \frac{0^k}{(k+1)!} = I$.
\subsubsection{(f)}
Let $s \in \textbf{SIM}(2)$. Then we can construct a $B \in \mathfrak{sim}(2)$ such that $e^B = s$.
We set $\lambda = \log(\alpha)$ and $\theta_B = \theta_s$.
Finally, we just need to show that we can find $u_B, v_B$ such that $ \binom{u_s}{v_s} = VU = \binom{u}{v}$.
\textbf{TODO!}

\subsection{B4}
\subsubsection{(a)} Checked. (I'm really not sure how to show any work, since the answer is in the assignment.)
\subsubsection{(b)}
We can express any element of $U$ as this linear combination of three linearly independent matrices:
\[ u_1\threematrix{0}{0}{0}{0}{0}{-1}{0}{1}{0} + 
   u_2\threematrix{0}{0}{1}{0}{0}{0} {-1}{0} {0} +
   u_3\threematrix{0}{-1}{0}{1}{0}{0}{0}{0}{0} \]
As we can see from setting $u_1 = u_2 = u_3 = 0$, this subspace of $M_3$ is a 3-dimensional vector space of real numbers, namely $\reals^3$.

We know thate these matrices are never invertible, since we can always find a nontrivial kernel (as happens in the next line).
Or we could calculate the determinant to 0, since $-u_3u_1u_2 + u_2u_3u_1 = 0$.

The kernel can be found by looking at (a), where we notice that each entry is $0$ precisely when $u_1 = v_1, u_2 = v_2, u_3 = v_3$.
(The kernel is one-dimensional, since row-reduction shows that $U$ has dimension 2).
Thus the kernel is ${\alpha(u_1, u_2, u_3) | \alpha \in \reals}$.
The linear map $U$ rotates its input $(v_1, v_2, v_3)$ to be orthogonal to both $(u_1, u_2, u_3)$ and $(v_1, v_2, v_3)$, and multiplies the length of the input vector by the length of $(u_1, u_2, u_3)$ and product of the sine of the angle between $(u_1, u_2, u_3)$.

If $(u_1, u_2, u_3) \cdot (v_1, v_2, v_3) = 0$, then $UV = $ \textbf{TODO!}

\subsection{(B5)}
\subsubsection{(a)}
\begin{eqnarray*}
  A^2 &=& 
  \left( \begin{array}{ccc}
  -b^2 - c^2 & ab & ac\\
  ab & -a^2 - c^2 & bc\\
  ac & bc & -a^2 - b^2
  \end{array}\right) = 
  \left( \begin{array}{ccc}
  a^2 -(a^2 + c^2 + b^2) & ab & ac\\
  ab & b^2 -(a^2 + c^2 + b^2)& bc\\
  ac & bc & c^2 -(a^2 + c^2 + b^2)
  \end{array}\right) \\
  &=& -\threematrix{(a^2+b^2+c^2)}{0}{0}{0}{(a^2+b^2+c^2)}{0}{0}{0}{(a^2+b^2+c^2)}
  + B = -\theta^2 + B
\end{eqnarray*}
When computing $AB$ and $BA$, each entry ends up being $abc - abc = 0$, so $AB = 0$.
Then, \[A^3 = A\cdot A^2 = A(-\theta^2I + B) = -\theta^2A + AB = -\theta^2A\].

\subsubsection{(b)}
\begin{eqnarray*}
  e^A &=&  I + \sum_{k=1}^\infty \frac{A^k}{k!} = 
  I + A + \frac{A^2}{2!} + \frac{A^3}{3!} + \frac{A^4}{4!} + \frac{A^5}{5!} + \dots \\
  &=& I + A + \frac{A^2}{2!} -\theta^2 \frac{A}{3!} -\theta^2 \frac{A^2}{4!} + \theta^4 \frac{A}{5!} +  \theta^4 \frac{A^2}{6!} \dots \\
  &=& I + \left(A - \theta^2 \frac{A}{3!} + \theta^4 \frac{A}{5!} + \dots \right) + \left( \frac{A^2}{2!} - \theta^2 \frac{A^2}{4!}  \theta^4 \frac{A^2}{6!} + \dots \right)\\
  &=& I + A\left(1 - \theta^2 \frac{1}{3!} + \theta^4 \frac{1}{5!} + \dots \right) + A^2\left( \frac{1^2}{2!} - \theta^2 \frac{1^2}{4!}  \theta^4 \frac{1^2}{6!} + \dots \right)
 \end{eqnarray*}
 As in question B2, we note that 
$\frac{\sin(\theta)}{\theta} = 1 - \frac{\theta^2}{3!} + \frac{\theta^5}{5!} - \dots $.
And
$\frac{1 - \cos(\theta)}{\theta^2} = \frac{1}{2!} - \frac{\theta^2}{4!} + \frac{\theta^4}{6!} - \dots $

So we have $e^A = I + \frac{\sin(\theta)}{\theta}A + \frac{1-\cos(\theta)}{\theta^2}A^2$.
Given that did the \textsl{same exact computation} as in B2, we unsurprisingly got the same result!

\subsubsection{(c)}
Given that $A^3 = -\theta^2A$, we notice that $A^3 + \theta^2A = 0$.
Its characteristic polynomial happens to be $\lambda^3 + (a^2 + b^2 + c^2)\lambda = 0$, or $\lambda^3 + \theta^2\lambda = 0$.
This is easily solvable, and we find that $\lambda = 0, \pm i\theta$.

We can then find the eigenvalues of $e^A$ by pluggin in $A$'s eigenvalues to the formula for $e^A$.
This works because any eigenvector of $A$ is an eigenvector of $A^2$ (with the eigenvalue squared), and every vector is an eigenvector of $I$ with eigenvalue 1.

We therefore find that, for $\lambda=0$, we get the eigenvalue $1$;
for $\lambda=\pm i\theta$, we get
\[1 + \frac{\sin(\theta)}{\theta}(\pm i\theta) + \frac{1-\cos(\theta)}{\theta^2}(\pm i\theta)^2
= 1 \pm i\sin(\theta) - 1 + \cos(\theta) = \cos(\theta) \pm i\sin(\theta)
= e^{\pm i\theta}\]
And all three of these eigenvalues for $e^A$ have eigenvalue 1, which means that $e^A$ must be orthogonal, as it has a full set of distinct eigenvalues of unit length.

\subsubsection{(d)}
We'll just fill in the missing parts that aren't provided in the problem set.

\noindent (2)
To verify the formula for $\text{exp}^{-1}(R)$, we plug in $e^{\text{exp}^{-1}(R)}$ to get

\begin{eqnarray*}
e^{\text{exp}^{-1}(R)}&=& 
I + \frac{\sin(\theta)}{\theta} \frac{\theta}{2\sin(\theta)}(R - R^\top)
+ \frac{1-\cos(\theta)}{\theta^2} \left( \frac{\theta}{2\sin(\theta)}(R - R^\top) \right)^2
\end{eqnarray*}

\subsection{(B6)}

\subsubsection{(a)} First to show surjectivity, let $v \in \mathbb{R}^3$ be any vector. Then set $v^* = R^{-1} (v - W)$. Clearly $v^* \in \mathbb{R}^3$ since $W \in \mathbb{R}^3$ and $R^{-1}$ exists and has full rank, being a rotation matrix. Then

\[ \rho(v^*) = R v^* + W = R (R^{-1} (v - W)) + W = (v - w) + W = v \]

So $v \in Im \, (\rho) $

And if $\rho(v_1) = \rho(v_2)$, then
\[ Rv_1 + W = Rv_2 + W \Longrightarrow v_1 = v_2 \]

since we just subtract the $W's$ and multiply by $R^{-1}$. So $\rho$ is injective. 

Now we use the matrix representation to show that $\rho$ is a group, where the group multiplication is just matrix multiplication of the representative matrices of $\rho$, which is composition of $\rho$ . Let $\rho_1 \sim \left(\begin{array}{cc} R_1 & W_1 \\ 0 & 1 \end{array}\right)$ and $\rho_2 \sim \left( \begin{array}{cc} R_2 & W_2 \\ 0 & 1 \end{array}\right)$. Then 

\[\rho_1 \circ \rho_2 \sim \left(\begin{array}{cc} R_1 & W_1 \\ 0 & 1 \end{array}\right) \left( \begin{array}{cc} R_2 & W_2 \\ 0 & 1 \end{array}\right) = \left(\begin{array}{cc} R_1 R_2 & R_1 W_2 + W_1 \\ 0 & 1 \end{array}\right)\]

where we note that we can put the product of the blocks $R_1, R_2$ in the product matrix since the first three columns of the last row are all 0, so thus we just get rows of $R_1$ multiplied by columns of $R_2$. And similarly in computing the top right term we see that it is the sum $(R_1)_{i*} (W_2)_i + (W_1)_i$ over $i=1,2,3$, where $(R_1)_{i*}$ is the $i$th row of $R_1$. 

Since $R_1 R_2$ is still an orthogonal matrix, as orthogonal matrices form a group under multiplication, and $R_1 W_2 + W_1 \in \mathbb{R}^3$, then this is the representation of an affine map with $R = R_1 R_2$ and $W = R_1 2 + W_1$

Thus affine maps are closed under composition. 

The matrix $\left(\begin{array}{cc} R & W \\ 0 & 1 \end{array}\right)$ has determinant $|R| = 1$; we can see this by just doing the determinant expansion on the last row from the right. Thus this matrix is invertible, so it has an inverse (which must be a matrix of the same form by the closure property earlier). 

And the group identity is just the identity matrix formed by setting $R = I_2$ and $W = 0$. 

So this forms a group.

\subsubsection{(b)} 

Let $\vec{W} = (x,y,z)$ be a component decomposition. Then for a matrix $B$, we define the isomorphism  in the canonical way by 
\[ \phi : (B, *) \longrightarrow (\mathbb{R}^6, +) \]
\[ \phi(B) \longrightarrow (a,b,c,x,y,z) \]

So adding two matrices of the form $B$ is just adding the parameters, and there are exactly six such parameters. We can choose them arbitrarily to represent any vector in $\mathbb{R}^6$. Injectivity of $\phi$ is obvious. Thus this is an isomorphism. 

For two matrices $B_1, B_2$ with corresponding parameters $(a_1,b_1,c_1,x_1,y_1,z_1)$ and $(a_2,\cdots ,z_2)$, we have
\[ B_1 * B_2 = \left( \begin{array}{cccc} 0 & -c_1 & b_1 & x_1 \\ c_1 & 0 & -a_1 & y_1 \\ -b_1 & a_1 & 0 & z_1 \\ 0 & 0 & 0 & 0 \end{array}\right) \left( \begin{array}{cccc} 0 & -c_2 & b_2 & x_2 \\ c_2 & 0 & -a_2 & y_2 \\ -b_2 & a_2 & 0 & z_2 \\ 0 & 0 & 0 & 0 \end{array}\right)\]

\[ = \left( \begin{array}{cccc} -c_1 c_2 - b_1 b_2 & b_1 a_2 & c_1 a_2 & -c_1 y_2 + b_1 z_2 \\ -a_1 b_2 & -c_1 c_2 - a_1 a_2 & c_1 b_2 & c_1 x_2 - a_1 z_2 \\ \cdots & \cdots & \cdots & \cdots \end{array}\right)\]

Well these are just the first two rows of the resulting product matrix, but we can already see that if, for example, $b_1 a_2 \neq b_2 a_1$, then the second entry of the first row will be different if we reverse the order of multiplication. Thus in general these matrices do not commute. 

\subsubsection{(c)} 

Let $B_1, B_2$ be two matrices of the form $B$, with $\Omega_1,W_1$ and $\Omega_2,W_2$ as the blocks. Then

\[ B_1 B_2 = \left(\begin{array}{cc} \Omega_1 \Omega_2 & \Omega_1 W_2 \\ 0 & 0 \end{array}\right) \]

where, similarly as in part (b), since the last row of $B_2$ is zero, then we are just multiplying the first three columns of $B_1$ with the first three rows of $B_2$. So we get $\Omega_1 \Omega_2$ in the first 3-by-3 block of the product matrix, and then we get the first 3-by-3 block of $B_1$ multiplied by the last column of $B_2$, which is just $\Omega_1 W_2$.

Thus, we have 

\[ BB = \left( \begin{array}{cc} \Omega^2 & \Omega W \\ 0 & 0 \end{array}\right)\]

and inductively, if the formula holds for $B^{n-1}$, then

\[ B^{n-1} B = \left(\begin{array}{cc} \Omega^n & \Omega^{n-1} \\ 0 & 0 \end{array}\right) \]

Now we have 

\[ e^B = I_4 + \displaystyle\sum_{n=1}^{\infty} \dfrac{1}{n!} B^n = I_4 + \displaystyle\sum_{n=1}^{\infty} \dfrac{1}{n!} \left(\begin{array}{cc} \Omega^n & \Omega^{n-1}W \\ 0 & 0 \end{array}\right) = \left( \begin{array}{cc} e^{\Omega} & VW \\ 0 & 1 \end{array}\right)\]

with $V$ given by

\[V = \displaystyle\sum_{n=1}^{\infty} \dfrac{1}{n!} \Omega^{n-1} = \Omega^0  + \displaystyle\sum_{n=2}^{\infty} \dfrac{1}{n!} \Omega^{n-1} = I_3 + \displaystyle\sum_{k\ge 1} \dfrac{1}{(k+1)!} \Omega^k\]

where we just substitute $k=n-1$ to get the last expression. 

Now suppose $\sqrt{a^2 + b^2 + c^2} = 0$. Then we compute the powers of $\Omega$ to get: 

\[ \Omega^2 = \left(\begin{array}{ccc} -c^2 - b^2 & ab & ac \\ ab & -c^2 - a^2 & bc \\ ac & bc & -b^2 - a^2 \end{array}\right) = \left(\begin{array}{ccc} a^2 & ab & ac \\ ab & b^2 & bc \\ ac & bc & c^2 \end{array}\right) \]

\[ \Omega^3 = \left(\begin{array}{ccc} a^2 & ab & ac \\ ab & b^2 & bc \\ ac & bc & c^2 \end{array}\right) \left(\begin{array}{ccc} 0 & -c & b \\ c & 0 & -a \\ -b & a & 0 \end{array}\right) = \left(\begin{array}{ccc} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{array}\right) \]

Thus all powers $\Omega^n$ with $n > 2$ are 0, so we are left with
\[ e^{\Omega} = I_3 + \Omega + \dfrac{1}{2} \Omega^2 \]

which should just be $I_3$??

\subsubsection{(d)} 

For general $\theta$, we have

\[\Omega^3 = (a^2 + b^2 + c^2) \left(\begin{array}{ccc} 0 & c & -b \\ -c & 0 & a \\ b & -a & 0 \end{array}\right) = -\theta^2 \Omega \]

\[\Omega^4 = -(a^2 + b^2 + c^2) \Omega^2 = -\theta^2 \Omega^2 \]

\[ \Omega^5 = \theta^4 \Omega\]

and so forth. thus we can break up the exponential according to the parity of the power. We get

\[e^{\Omega} = I_3 + \displaystyle\sum_{n=1}^{\infty} \dfrac{1}{(2n-1)!} (-\theta^2)^{n-1} \Omega + \displaystyle\sum_{n=1}^{\infty} \dfrac{1}{(2n)!} (-\theta^2)^{n-1} \Omega^2\]

and the coefficients of $\Omega, \Omega^2$ are the power series expansions for 



\end{document}
