\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\setcounter{secnumdepth}{0}
\newcommand{\emptyspace}{\{0\}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\ints}{\mathbb{Z}}
\newcommand{\trace}{\textrm{tr}}
\newcommand{\simtwo}{\textbf{SIM(2)}}
\newcommand{\norm}[1]{||#1||}
\newcommand{\bignorm}[1]{\left|\left|#1\right|\right|}
\newcommand{\twomatrix}[4]{\left(\begin{array}{cc} #1 & #2\\ #3 & #4 \end{array}\right)}
\newcommand{\threematrix}[9]{\left(\begin{array}{ccc} #1 & #2 & #3\\ #4 & #5 & #6 \\ #7 & #8 & #9 \end{array}\right)}

\begin{document}

\begin{center}CIS 515 --- HW5\\Sam Panzer and Kevin Shi\end{center}
\subsection{B1}
Let $\alpha =\norm{A} \geq 0$, and note that $\norm{A^p} \leq \norm{A}^p$ by the definition of a matrix norm.
Then 
\begin{eqnarray*}
\bignorm{I + \sum_{k=1}^m \frac{A^k}{k!}} &\leq&
\norm{I} + \sum_{k=1}^m \frac{\norm{A^k}}{k!} \text{,  by triangle inequality }\\
&\leq& 1 + \sum_{k=1}^m \frac{\alpha^k}{k!} = \sum_{k=0}^m \frac{\alpha^k}{k!}\\
&\leq& \sum_{k=1}^\infty \frac{\alpha^k}{k!} = e^\alpha = e^{\norm{A}}
\end{eqnarray*}
Thus
\[ \bignorm{I + \sum_{k=1}^m \frac{A^k}{k!}} \leq e^\norm{A}\]
And since this sum is monotonic increasing and bounded above, it must have a limit.
Since the norms of the matrices in the sequence $E_m$ converge, the sequence itself must converge.
Therefore it is well-defined to denote the limit of this sequence by $e^A$.

\subsection{B2}
\subsubsection{(a)}
Note that these matrices are spanned by this linear combination:
\[
\theta \left(\begin{array}{ccc}
  0 & -1 & 0\\
  1 & 0 & 0 \\
  0 & 0 & 0 
\end{array}\right) 
+
u \left(\begin{array}{ccc}
  0 & 0 & 1\\
  0 & 0 & 0 \\
  0 & 0 & 0 
\end{array}\right) 
+
v \left(\begin{array}{ccc}
  0 & 0 & 0\\
  0 & 0 & 1 \\
  0 & 0 & 0 
\end{array}\right) 
\]
And the three component matrices are clearly linearly independent, because they do not share any nonzero entries.
%It's also clear that adding two such matrices, or multiplying them by an element of $\reals$ gives another vector in this set; the matrices $0$ and $I$ inherit their behavior from $M_3(\reals)$
Thus this is a subspace of $M_3(\reals)$ and isomorphic to $(\reals^3,+)$

Now consider $B = (\theta, u, v)$ and $C = (\theta', u', v')$.  Then 
\[
BC = 
\left(\begin{array}{ccc}
  -\theta\theta' & 0 & -\theta v'\\
  0 & -\theta \theta' & -\theta u' \\
  0 & 0 & 0 
\end{array}\right)  \text{ and }
CB = 
\left(\begin{array}{ccc}
  -\theta\theta' & 0 & -\theta' v\\
  0 & -\theta \theta' & -\theta' u \\
  0 & 0 & 0 
\end{array}\right)
\]
Note that the primes switched places, which shows that $BC$ is not necessarily equal to $CB$.

\subsubsection{(b)}
First, let $\theta = 0$. We then have that 
\[ B = 
\left(\begin{array}{ccc}
  0 & 0 & u \\
  0 & 0 & v \\
  0 & 0 & 0 
\end{array}\right) 
\]
Note that $B^2 = 0$. We immediately see that 
\[ e^B = I + \sum_{k=1}^\infty \frac{B^k}{k!} = 
         I + B + \sum_{k=2}^\infty \frac{B^k}{k!} =
         I + B + \sum_{k=2}^\infty \frac{0}{k!} = I + B
\]

For the second case ($\theta \neq 0$), we do a similar simplification.
\[
B^3 = B\cdot B^2 = B \cdot 
\left(\begin{array}{ccc}
  -\theta & 0 & -v\theta \\
  0 & -\theta & -u\theta \\
  0 & 0 & 0 
\end{array}\right) 
=
\left(\begin{array}{ccc}
  0 & -\theta ^2 & -u\theta^2 \\
  -\theta^2 & 0 & -v\theta^2 \\
  0 & 0 & 0 
\end{array}\right) 
= -\theta^2 B
\]
Then
\begin{eqnarray*}
  e^B &=&  I + B + \frac{B^2}{2!} + \frac{B^3}{3!} + \frac{B^4}{4!} + \frac{B^5}{5!} + \frac{B^6}{6!} + \dots \\
  &=&  I + B + \frac{B^2}{2!} + \frac{-\theta^2 B}{3!} + \frac{-\theta^2 B^2}{4!} + \frac{\theta^4 B}{5!} + \frac{\theta^4 B^2}{6!} + \dots \\
  &=&  I + \left(B - \frac{\theta^2 B}{3!} + \frac{\theta^4 B}{5!} - \dots \right) + 
           \left(\frac{B^2}{2!} - \frac{\theta^2 B}{4!} + \frac{\theta^4 B}{6!} - \dots \right)\\
  &=&  I + B \left(1 - \frac{\theta^2}{3!} + \frac{\theta^4}{5!} - \dots \right) + 
           B^2\left(\frac{1}{2!} - \frac{\theta^2}{4!} + \frac{\theta^4}{6!} - \dots \right)\\
\end{eqnarray*}
Note that $\sin(\theta) = \theta - \frac{\theta^3}{3!} + \frac{\theta^5}{5!} - \dots $,
which means that $\frac{\sin(\theta)}{\theta} = 1 - \frac{\theta^2}{3!} + \frac{\theta^5}{5!} - \dots $

\noindent
Similarly, $\cos(\theta) = 1 - \frac{\theta^2}{2!} + \frac{\theta^4}{4!} - \dots $, so
$\frac{1 - \cos(\theta)}{\theta^2} = \frac{1}{2!} - \frac{\theta^2}{4!} + \frac{\theta^4}{6!} - \dots $

\noindent
Thus we have
\[e^B = I + B\frac{\sin(\theta)}{\theta} + B^2 \frac{1 - \cos(\theta)}{\theta^2} \text{, or }\]
\[
  e^B = 
\left(\begin{array}{ccc}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 
\end{array}\right) 
+
\left(\begin{array}{ccc}
  0 & -\sin(\theta) & u\frac{\sin(\theta)}{\theta} \\
  -\sin(\theta) & 0 & v\frac{\sin(\theta)}{\theta} \\
  0 & 0 & 0 
\end{array}\right) 
+
\left(\begin{array}{ccc}
  \cos(\theta) - 1 & 0 & v\frac{\cos(\theta) - 1}{\theta} \\
  0 & \cos(\theta) - 1 & u\frac{\cos(\theta) - 1}{\theta} \\
  0 & 0 & 0 
\end{array}\right) 
\]
which gives the desired result.

\subsubsection{(c)}
The form of $e^B$ in the previous section confirms that $e^B$ is in \textbf{SE}$(2)$.
To show injectivity, we let $w_1,w_2 \in \reals$.
Then all we need to show is that, given $\theta$, we can find $u$ and $v$ such that
$u\frac{\sin(\theta)}{\theta} + v \frac{\cos(\theta) - 1}{\theta} = w_1$ and
$u\frac{1-\cos(\theta)}{\theta} + v \frac{\sin(\theta)}{\theta} = w_2$.
This is equivalent to solving the system
\[
\left(\begin{array}{cc}
\frac{\sin(\theta)}{\theta} & \frac{\cos(\theta) - 1}{\theta} \\
\frac{1-\cos(\theta)}{\theta} & \frac{\sin(\theta)}{\theta}
\end{array}\right)
\binom{u}{v} = \binom{w_1}{w_2} % Abuse of notation? sure!
\]
Which is of course solvable with exactly one solution when the determinant of the $2\times2$ matrix $T$ is nonzero.
Well, we end up with $\det(T) = -\frac{2\cos(\theta)}{\theta}$, which is nonzero (and defined) so long as $\theta \neq k\pi + \frac{\pi}{2}, k \in \ints$ (and we already assumed that $\theta \neq 0$).
So our map is injective so long as $\theta$ isn't a half-multiple of $\pi$.

\subsection{B3}
\subsubsection{(a)}
The group operation is composition, and the identity is the map where $\alpha = 1, \theta = w_1=w_2 =0,$.

Now, we just need to show that if $R\circ S \in$ \simtwo for any $R,S \in$ \simtwo.
Let $R$ have parameters $(\theta_1, w_1, z_1, \alpha_1)$ and $S$ have parameters $(\theta_1, w_1, z_1, \alpha_1)$. Then
\begin{eqnarray*}
  (R \circ S)\binom{x}{y} &=&  R(S\binom{x}{y}) \\
  &=& \alpha_2 
    \left(\begin{array}{cc}
      \cos(\theta_2) & -\sin(\theta_2)\\
      \sin(\theta_2) & \cos(\theta_2)
    \end{array}\right) 
  \left(\alpha_1 
    \left(\begin{array}{cc}
      \cos(\theta_1) & -\sin(\theta_1)\\
      \sin(\theta_1) & \cos(\theta_1)
    \end{array}\right)\binom{x}{y} + \binom{w_1}{z_1}\right) + \binom{w_2}{z_2}\\
&=& \alpha_1\alpha_2 \left(\begin{array}{cc}
      \cos(\theta_1)\cos(\theta_2) -\sin(\theta_1)\sin(\theta_2)& 
      -\sin(\theta_1)\cos(\theta_2) -\cos(\theta_1)\sin(\theta_2)\\
      \cos(\theta_1)\sin(\theta_2) +\sin(\theta_1)\cos(\theta_2)& 
      -\sin(\theta_1)\sin(\theta_2) +\cos(\theta_1)\cos(\theta_2)\\
    \end{array}\right)\binom{x}{y}\\
    &&+
  \left(\begin{array}{cc}
      \alpha_2\cos(\theta_2)w_1 -\alpha_2\sin(\theta_2)z_1 + w_2 \\
      \alpha_2\sin(\theta_2)w_1 +\alpha_2\sin(\theta_2)z_1 + z_2 \\
    \end{array}\right) \\
    &=& 
  \alpha_1\alpha_2\left(\begin{array}{cc}
      \cos(\theta_1 + \theta_2) & -\sin(\theta_1 + \theta_2)\\
      \sin(\theta_1 + \theta_2) & \cos(\theta_1 + \theta_2)
    \end{array}\right)\binom{x}{y} +
  \left(\begin{array}{cc}
      \alpha_2\cos(\theta_2)w_1 -\alpha_2\sin(\theta_2)z_1 + w_2 \\
      \alpha_2\sin(\theta_2)w_1 +\alpha_2\cos(\theta_2)z_1 + z_2 \\
    \end{array}\right) \\
\end{eqnarray*}
Which is also in \simtwo.
This formulation shows that for $R$ to be $S^{-1}$, we need $R$'s parameters to be
\begin{eqnarray*}
  \theta_2 &=&  -\theta_1\\
  w_1 &=& \alpha_1^{-1}(\sin(-\theta_1)z_1 - \cos(-\theta_1)w_1)\\
  z_1 &=& \alpha_1^{-1}(\cos(-\theta_1)z_1 + \cos(-\theta_1)w_1)\\
  \alpha_2 &=&  \alpha_1^{-1}
\end{eqnarray*}
Substituting this into the above equality gives $(R \circ S) \binom{x}{y} = \binom{x}{y}$, the identity element, so $R = S^{-1}$
\subsubsection{(b)}
We're looking at a subspace of $\reals^9$, such that there are only four distinct parameters.
We can therefore express any element of $\mathfrak{sim}(2)$ as
\[
\lambda \left(\begin{array}{ccc}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 0
    \end{array}\right) 
 + \theta \left(\begin{array}{ccc}
      0 & -1 & 0\\
      1 & 0 & 0\\
      0 & 0 & 0
    \end{array}\right) 
 + u\left(\begin{array}{ccc}
      0 & 0 & 1\\
      0 & 0 & 0\\
      0 & 0 & 0
    \end{array}\right) 
 + v\left(\begin{array}{ccc}
      0 & 0 & 0\\
      0 & 0 & 1\\
      0 & 0 & 0
    \end{array}\right) 
\]
This makes it clear that the four matrices span $\mathfrak{sim}(2)$; they are also clearly linearly independent.
We therefore have a simple bijection between $\mathfrak{sim}(2)$ and $\reals^4$.
Thus $\mathfrak{sim}(2)$ is isomorphic to $\reals^4$.
\subsubsection{(c)}
If we let follow the suggestion and let $\Omega$ and $J$ be defined as in the assignment,
we can verify that $J^2 = -I$ by high technology.
Now, we need to show this by induction on $k$:
\[ \Omega^k = \frac{1}{2}\left((\lambda + i\theta)^k + (\lambda + i\theta)^k \right)I
  + \frac{1}{2i}\left((\lambda + i\theta)^k - (\lambda + i\theta)^k \right) J\]
We start with a base case of $k=0$, using the convention that $A^0 = I$ for any square matrix $A$:
For the base case, $\Omega^0 = I$, so we just need to show that the expression evaluates to $I$.
\[
  \frac{1}{2}\left((\lambda + i\theta)^0 + (\lambda + i\theta)^0 \right)
  + \frac{1}{2i}\left((\lambda + i\theta)^0 - (\lambda + i\theta)^0 \right) = \frac{1}{2} (2I) + \frac{1}{2i}(0)J = I
\]

We now assume that that this equality holds for $k$, and try to show tha it is also true for $k+1$.
\begin{eqnarray*}
\Omega^{k+1} &=&  \Omega^k \cdot \Omega\\
&=& \left( \frac{1}{2}\left((\lambda + i\theta)^k + (\lambda + i\theta)^k \right)I
  + \frac{1}{2i}\left((\lambda + i\theta)^k - (\lambda + i\theta)^k \right)J \right)(\lambda I + \theta J)\\
  &=& \frac{1}{2}\lambda\left( (\lambda + i\theta)^k + (\lambda - i\theta)^k \right)I  
     - \frac{1}{2i}\theta\left( (\lambda - i\theta)^k - (\lambda - i\theta)^k \right)I +\\
  &&  \frac{1}{2i}\lambda\left( (\lambda + i\theta)^k + (\lambda - i\theta)^k \right)J +
      \frac{1}{2}\theta\left( (\lambda + i\theta)^k + (\lambda - i\theta)^k \right)J\\
  &=& \frac{1}{2}\left( \lambda(\lambda + i\theta) + \lambda(\lambda - i\theta)^k +
                        i\theta(\lambda + i\theta)^k - i\theta(\lambda-i\theta)\right)I + \\
  && \frac{1}{2i}\left( \lambda(\lambda + i\theta) - \lambda(\lambda - i\theta)^k +
                        i\theta(\lambda + i\theta)^k + i\theta(\lambda-i\theta)\right)J\\
  &=& \frac{1}{2}\left((\lambda + i\theta)(\lambda + i\theta)^k +
                       (\lambda - i\theta)(\lambda - i\theta)^k \right)I + \\
  && \frac{1}{2}\left((\lambda + i\theta)(\lambda + i\theta)^k -
                       (\lambda - i\theta)(\lambda - i\theta)^k \right)J \\
  &=& \frac{1}{2}\left((\lambda + i\theta)^{k+1} + (\lambda - i\theta)^{k+1} \right)I
  + \frac{1}{2i}\left((\lambda + i\theta)^{k+1} - (\lambda + i\theta)^{k+1} \right) J
\end{eqnarray*}
With that done, we can now look at
\begin{eqnarray*}
  e^\Omega &=&  \sum_{k=0}^\infty\frac{1}{k!}\Omega^k\\
  &=& \sum_{k=0}^\infty \frac{1}{k!}\left[ \frac{1}{2}\left((\lambda + i\theta)^k + (\lambda - i\theta)^k \right)I
    +\frac{1}{2i}\left((\lambda + i\theta)^k - (\lambda + i\theta)^k \right)J \right]\\
    &=& \frac{1}{2}I\left( \sum_{k=0}^\infty \frac{1}{k!}(\lambda + i\theta)^k +
            \sum_{k=0}^\infty \frac{1}{k!}(\lambda - i\theta)^k\right)
    +\frac{1}{2i}J\left(\sum_{k=0}^\infty \frac{1}{k!}(\lambda + i\theta)^k - 
    \sum_{k=1}^\infty \frac{1}{k!}(\lambda + i\theta)^k \right)\\
    &=& \frac{1}{2} \left( e^{\lambda + i\theta} + e^{\lambda - i\theta}  \right)I +
       \frac{1}{2i} \left( e^{\lambda + i\theta} - e^{\lambda - i\theta}  \right)J \\
    &=& e^\lambda(\cos(\theta)I + \sin(\theta)J)\\
    &=& e^\lambda\twomatrix{\cos(\theta)}{-\sin(\theta)}{\sin(\theta)}{\cos(\theta)}
\end{eqnarray*}
Whew!
\subsubsection{(d)}
We'll do this first proof by induction.
The base case is verified by $B^1 = \twomatrix{\Omega}{U}{0}{0}$. And the inductive case is
\[B^{n+1} = B^n\cdot B = \twomatrix{\Omega^n}{\Omega^{n-1} U}{0}{0} \twomatrix{\Omega}{U}{0}{0}
          = \twomatrix{\Omega^{n+1}}{ \Omega^n U}{0}{0}\]

Now, 
\begin{eqnarray*}
  e^B &=&  I + \sum_{k=1}^\infty \frac{1}{k!}\twomatrix{\Omega^k}{\Omega^{k-1}U}{0}{0}\\
  %&=& I + \sum_{k=1}^\infty \twomatrix{I +\frac{1}{k!}\Omega^k}{\frac{1}{k!}\Omega^{k-1}U}{0}{0}\\
  &=& \twomatrix{I + \sum_{k=1}^\infty \frac{1}{k!}\Omega^k}{\sum_{k=1}^\infty \frac{1}{k!}\Omega^{k-1}U}{0}{1}\\
\end{eqnarray*}
The upper-left entry is just the definition of $e^B$, and the upper-right entry is
\[
\sum_{k=1}^\infty \frac{1}{k!}\Omega^{k-1}U = I + \sum_{k=2}^\infty \frac{1}{k!}\Omega^{k-1}U 
= I + \sum_{k=1}^\infty \frac{1}{(k+1)!}\Omega^{k}U \text{ as desired }
\]

\subsubsection{(e)}
Let's start by unfolding the definition of the integral.
\begin{eqnarray*}
  \int_0^1 e^{\Omega t}dt &=& \int_0^1 Idt + \int_0^1 \left( \sum_{k=1}^\infty \frac{t^k\Omega^k}{k!} \right) dt\\
  &=& I + \sum_{k=1}^\infty\left( \int_0^1 \frac{t^k}{k!}\Omega^k \right)
  = I + \sum_{k=1}^\infty\left( \Omega^k \int_0^1 \frac{t^k}{k!}\right)\\
  &=&  I + \sum_{k=1}^\infty\left( \Omega^k \left[\frac{t^{k+1}}{(k+1)!}\right]_0^1\right)
  = I + \sum_{k=1}^\infty \frac{\Omega^k }{(k+1)!} = V
\end{eqnarray*}
This, of course, depends on the power series of a matrix being uniformly convergent, since we switch the integral and summation on the second line.
I will assume that power series of matrices are uniformly convergent just as power series of complex numbers are, and that the result I'm relying on (that we can switch these operations) goes through without a hitch.

Then, if $\theta = \lambda = 0$, then $\Omega = 0$, so $V = I + \sum_{k=1}^\infty \frac{0^k}{(k+1)!} = I$.
\subsubsection{(f)}
Let $s \in \textbf{SIM}(2)$. Then we can construct a $B \in \mathfrak{sim}(2)$ such that $e^B = s$.
We set $\lambda = \log(\alpha)$ and $\theta_B = \theta_s$.
Finally, we just need to show that we can find $u_B, v_B$ such that $ \binom{u_s}{v_s} = VU = \binom{u}{v}$.
\textbf{TODO!}

\subsection{B4}
\subsubsection{(a)} Checked. (I'm really not sure how to show any work, since the answer is in the assignment.)
\subsubsection{(b)}
We can express any element of $U$ as this linear combination of three linearly independent matrices:
\[ u_1\threematrix{0}{0}{0}{0}{0}{-1}{0}{1}{0} + 
   u_2\threematrix{0}{0}{1}{0}{0}{0} {-1}{0} {0} +
   u_3\threematrix{0}{-1}{0}{1}{0}{0}{0}{0}{0} \]
As we can see from setting $u_1 = u_2 = u_3 = 0$, this subspace of $M_3$ is a 3-dimensional vector space of real numbers, namely $\reals^3$.

We know thate these matrices are never invertible, since we can always find a nontrivial kernel (as happens in the next line).
Or we could calculate the determinant to 0, since $-u_3u_1u_2 + u_2u_3u_1 = 0$.

The kernel can be found by looking at (a), where we notice that each entry is $0$ precisely when $u_1 = v_1, u_2 = v_2, u_3 = v_3$.
(The kernel is one-dimensional, since row-reduction shows that $U$ has dimension 2).
Thus the kernel is ${\alpha(u_1, u_2, u_3) | \alpha \in \reals}$.
The linear map $U$ rotates its input $(v_1, v_2, v_3)$ to be orthogonal to both $(u_1, u_2, u_3)$ and $(v_1, v_2, v_3)$, and multiplies the length of the input vector by the length of $(u_1, u_2, u_3)$ and product of the sine of the angle between $(u_1, u_2, u_3)$.

If $(u_1, u_2, u_3) \cdot (v_1, v_2, v_3) = 0$, then $UV = $ \textbf{TODO!}

\subsection{(B5)}
\subsubsection{(a)}
\begin{eqnarray*}
  A^2 &=& 
  \left( \begin{array}{ccc}
  -b^2 - c^2 & ab & ac\\
  ab & -a^2 - c^2 & bc\\
  ac & bc & -a^2 - b^2
  \end{array}\right) = 
  \left( \begin{array}{ccc}
  a^2 -(a^2 + c^2 + b^2) & ab & ac\\
  ab & b^2 -(a^2 + c^2 + b^2)& bc\\
  ac & bc & c^2 -(a^2 + c^2 + b^2)
  \end{array}\right) \\
  &=& -\threematrix{(a^2+b^2+c^2)}{0}{0}{0}{(a^2+b^2+c^2)}{0}{0}{0}{(a^2+b^2+c^2)}
  + B = -\theta^2 + B
\end{eqnarray*}
When computing $AB$ and $BA$, each entry ends up being $abc - abc = 0$, so $AB = 0$.
Then, \[A^3 = A\cdot A^2 = A(-\theta^2I + B) = -\theta^2A + AB = -\theta^2A\].

\subsubsection{(b)}
\begin{eqnarray*}
  e^A &=&  I + \sum_{k=1}^\infty \frac{A^k}{k!} = 
  I + A + \frac{A^2}{2!} + \frac{A^3}{3!} + \frac{A^4}{4!} + \frac{A^5}{5!} + \dots \\
  &=& I + A + \frac{A^2}{2!} -\theta^2 \frac{A}{3!} -\theta^2 \frac{A^2}{4!} + \theta^4 \frac{A}{5!} +  \theta^4 \frac{A^2}{6!} \dots \\
  &=& I + \left(A - \theta^2 \frac{A}{3!} + \theta^4 \frac{A}{5!} + \dots \right) + \left( \frac{A^2}{2!} - \theta^2 \frac{A^2}{4!}  \theta^4 \frac{A^2}{6!} + \dots \right)\\
  &=& I + A\left(1 - \theta^2 \frac{1}{3!} + \theta^4 \frac{1}{5!} + \dots \right) + A^2\left( \frac{1^2}{2!} - \theta^2 \frac{1^2}{4!}  \theta^4 \frac{1^2}{6!} + \dots \right)
 \end{eqnarray*}
 As in question B2, we note that 
$\frac{\sin(\theta)}{\theta} = 1 - \frac{\theta^2}{3!} + \frac{\theta^5}{5!} - \dots $.
And
$\frac{1 - \cos(\theta)}{\theta^2} = \frac{1}{2!} - \frac{\theta^2}{4!} + \frac{\theta^4}{6!} - \dots $

So we have $e^A = I + \frac{\sin(\theta)}{\theta}A + \frac{1-\cos(\theta)}{\theta^2}A^2$.
Given that did the \textsl{same exact computation} as in B2, we unsurprisingly got the same result!

\subsubsection{(c)}
Given that $A^3 = -\theta^2A$, we notice that $A^3 + \theta^2A = 0$.
Its characteristic polynomial happens to be $\lambda^3 + (a^2 + b^2 + c^2)\lambda = 0$, or $\lambda^3 + \theta^2\lambda = 0$.
This is easily solvable, and we find that $\lambda = 0, \pm i\theta$.

We can then find the eigenvalues of $e^A$ by pluggin in $A$'s eigenvalues to the formula for $e^A$.
This works because any eigenvector of $A$ is an eigenvector of $A^2$ (with the eigenvalue squared), and every vector is an eigenvector of $I$ with eigenvalue 1.

We therefore find that, for $\lambda=0$, we get the eigenvalue $1$;
for $\lambda=\pm i\theta$, we get
\[1 + \frac{\sin(\theta)}{\theta}(\pm i\theta) + \frac{1-\cos(\theta)}{\theta^2}(\pm i\theta)^2
= 1 \pm i\sin(\theta) - 1 + \cos(\theta) = \cos(\theta) \pm i\sin(\theta)
= e^{\pm i\theta}\]
And all three of these eigenvalues for $e^A$ have eigenvalue 1, which means that $e^A$ must be orthogonal, as it has a full set of distinct eigenvalues of unit length.

\subsubsection{(d)}
We'll just fill in the missing parts that aren't provided in the problem set.

\noindent (2)
To verify the formula for $\text{exp}^{-1}(R)$, we plug in $e^{\text{exp}^{-1}(R)}$ to get

\begin{eqnarray*}
e^{\text{exp}^{-1}(R)}&=& 
I + \frac{\sin(\theta)}{\theta} \frac{\theta}{2\sin(\theta)}(R - R^\top)
+ \frac{1-\cos(\theta)}{\theta^2} \left( \frac{\theta}{2\sin(\theta)}(R - R^\top) \right)^2
\end{eqnarray*}

\end{document}
