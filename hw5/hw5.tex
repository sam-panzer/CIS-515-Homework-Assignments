\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\setcounter{secnumdepth}{0}
\newcommand{\emptyspace}{\{0\}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\ints}{\mathbb{Z}}
\newcommand{\trace}{\textrm{tr}}
\newcommand{\simtwo}{\textbf{SIM(2)}}
\newcommand{\norm}[1]{||#1||}
\newcommand{\bignorm}[1]{\left|\left|#1\right|\right|}
\newcommand{\twomatrix}[4]{\left(\begin{array}{cc} #1 & #2\\ #3 & #4 \end{array}\right)}
\newcommand{\threematrix}[9]{\left(\begin{array}{ccc} #1 & #2 & #3\\ #4 & #5 & #6 \\ #7 & #8 & #9 \end{array}\right)}

\begin{document}

\begin{center}CIS 515 --- HW5\\Sam Panzer and Kevin Shi\end{center}
\subsection{B1}
Let $\alpha =\norm{A} \geq 0$, and note that $\norm{A^p} \leq \norm{A}^p$ by the definition of a matrix norm.
Then 
\begin{eqnarray*}
\bignorm{I + \sum_{k=1}^m \frac{A^k}{k!}} &\leq&
\norm{I} + \sum_{k=1}^m \frac{\norm{A^k}}{k!} \text{,  by triangle inequality }\\
&\leq& 1 + \sum_{k=1}^m \frac{\alpha^k}{k!} = \sum_{k=0}^m \frac{\alpha^k}{k!}\\
&\leq& \sum_{k=1}^\infty \frac{\alpha^k}{k!} = e^\alpha = e^{\norm{A}}
\end{eqnarray*}
Thus
\[ \bignorm{I + \sum_{k=1}^m \frac{A^k}{k!}} \leq e^\norm{A}\]
And since this sum is monotonic increasing and bounded above, it must have a limit.
Since the norms of the matrices in the sequence $E_m$ converge, the sequence itself must converge.
Therefore it is well-defined to denote the limit of this sequence by $e^A$.

\subsection{B2}
\subsubsection{(a)}
Note that these matrices are spanned by this linear combination:
\[
\theta \left(\begin{array}{ccc}
  0 & -1 & 0\\
  1 & 0 & 0 \\
  0 & 0 & 0 
\end{array}\right) 
+
u \left(\begin{array}{ccc}
  0 & 0 & 1\\
  0 & 0 & 0 \\
  0 & 0 & 0 
\end{array}\right) 
+
v \left(\begin{array}{ccc}
  0 & 0 & 0\\
  0 & 0 & 1 \\
  0 & 0 & 0 
\end{array}\right) 
\]
And the three component matrices are clearly linearly independent, because they do not share any nonzero entries.
%It's also clear that adding two such matrices, or multiplying them by an element of $\reals$ gives another vector in this set; the matrices $0$ and $I$ inherit their behavior from $M_3(\reals)$
Thus this is a subspace of $M_3(\reals)$ and isomorphic to $(\reals^3,+)$

Now consider $B = (\theta, u, v)$ and $C = (\theta', u', v')$.  Then 
\[
BC = 
\left(\begin{array}{ccc}
  -\theta\theta' & 0 & -\theta v'\\
  0 & -\theta \theta' & -\theta u' \\
  0 & 0 & 0 
\end{array}\right)  \text{ and }
CB = 
\left(\begin{array}{ccc}
  -\theta\theta' & 0 & -\theta' v\\
  0 & -\theta \theta' & -\theta' u \\
  0 & 0 & 0 
\end{array}\right)
\]
Note that the primes switched places, which shows that $BC$ is not necessarily equal to $CB$.

\subsubsection{(b)}
First, let $\theta = 0$. We then have that 
\[ B = 
\left(\begin{array}{ccc}
  0 & 0 & u \\
  0 & 0 & v \\
  0 & 0 & 0 
\end{array}\right) 
\]
Note that $B^2 = 0$. We immediately see that 
\[ e^B = I + \sum_{k=1}^\infty \frac{B^k}{k!} = 
         I + B + \sum_{k=2}^\infty \frac{B^k}{k!} =
         I + B + \sum_{k=2}^\infty \frac{0}{k!} = I + B
\]

For the second case ($\theta \neq 0$), we do a similar simplification.
\[
B^3 = B\cdot B^2 = B \cdot 
\left(\begin{array}{ccc}
  -\theta & 0 & -v\theta \\
  0 & -\theta & -u\theta \\
  0 & 0 & 0 
\end{array}\right) 
=
\left(\begin{array}{ccc}
  0 & -\theta ^2 & -u\theta^2 \\
  -\theta^2 & 0 & -v\theta^2 \\
  0 & 0 & 0 
\end{array}\right) 
= -\theta^2 B
\]
Then
\begin{eqnarray*}
  e^B &=&  I + B + \frac{B^2}{2!} + \frac{B^3}{3!} + \frac{B^4}{4!} + \frac{B^5}{5!} + \frac{B^6}{6!} + \dots \\
  &=&  I + B + \frac{B^2}{2!} + \frac{-\theta^2 B}{3!} + \frac{-\theta^2 B^2}{4!} + \frac{\theta^4 B}{5!} + \frac{\theta^4 B^2}{6!} + \dots \\
  &=&  I + \left(B - \frac{\theta^2 B}{3!} + \frac{\theta^4 B}{5!} - \dots \right) + 
           \left(\frac{B^2}{2!} - \frac{\theta^2 B}{4!} + \frac{\theta^4 B}{6!} - \dots \right)\\
  &=&  I + B \left(1 - \frac{\theta^2}{3!} + \frac{\theta^4}{5!} - \dots \right) + 
           B^2\left(\frac{1}{2!} - \frac{\theta^2}{4!} + \frac{\theta^4}{6!} - \dots \right)\\
\end{eqnarray*}
Note that $\sin(\theta) = \theta - \frac{\theta^3}{3!} + \frac{\theta^5}{5!} - \dots $,
which means that $\frac{\sin(\theta)}{\theta} = 1 - \frac{\theta^2}{3!} + \frac{\theta^5}{5!} - \dots $

\noindent
Similarly, $\cos(\theta) = 1 - \frac{\theta^2}{2!} + \frac{\theta^4}{4!} - \dots $, so
$\frac{1 - \cos(\theta)}{\theta^2} = \frac{1}{2!} - \frac{\theta^2}{4!} + \frac{\theta^4}{6!} - \dots $

\noindent
Thus we have
\[e^B = I + B\frac{\sin(\theta)}{\theta} + B^2 \frac{1 - \cos(\theta)}{\theta^2} \text{, or }\]
\[
  e^B = 
\left(\begin{array}{ccc}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 
\end{array}\right) 
+
\left(\begin{array}{ccc}
  0 & -\sin(\theta) & u\frac{\sin(\theta)}{\theta} \\
  -\sin(\theta) & 0 & v\frac{\sin(\theta)}{\theta} \\
  0 & 0 & 0 
\end{array}\right) 
+
\left(\begin{array}{ccc}
  \cos(\theta) - 1 & 0 & v\frac{\cos(\theta) - 1}{\theta} \\
  0 & \cos(\theta) - 1 & u\frac{\cos(\theta) - 1}{\theta} \\
  0 & 0 & 0 
\end{array}\right) 
\]
which gives the desired result.

\subsubsection{(c)}
The form of $e^B$ in the previous section confirms that $e^B$ is in \textbf{SE}$(2)$.
To show injectivity, we let $w_1,w_2 \in \reals$.
Then all we need to show is that, given $\theta$, we can find $u$ and $v$ such that
$u\frac{\sin(\theta)}{\theta} + v \frac{\cos(\theta) - 1}{\theta} = w_1$ and
$u\frac{1-\cos(\theta)}{\theta} + v \frac{\sin(\theta)}{\theta} = w_2$.
This is equivalent to solving the system
\[
\left(\begin{array}{cc}
\frac{\sin(\theta)}{\theta} & \frac{\cos(\theta) - 1}{\theta} \\
\frac{1-\cos(\theta)}{\theta} & \frac{\sin(\theta)}{\theta}
\end{array}\right)
\binom{u}{v} = \binom{w_1}{w_2} % Abuse of notation? sure!
\]
Which is of course solvable with exactly one solution when the determinant of the $2\times2$ matrix $T$ is nonzero.
Well, we end up with $\det(T) = -\frac{2\cos(\theta)}{\theta}$, which is nonzero (and defined) so long as $\theta \neq k\pi + \frac{\pi}{2}, k \in \ints$ (and we already assumed that $\theta \neq 0$).
So our map is injective so long as $\theta$ isn't a half-multiple of $\pi$.

\subsection{B3}
\subsubsection{(a)}
The group operation is composition, and the identity is the map where $\alpha = 1, \theta = w_1=w_2 =0,$.

Now, we just need to show that if $R\circ S \in$ \simtwo for any $R,S \in$ \simtwo.
Let $R$ have parameters $(\theta_1, w_1, z_1, \alpha_1)$ and $S$ have parameters $(\theta_1, w_1, z_1, \alpha_1)$. Then
\begin{eqnarray*}
  (R \circ S)\binom{x}{y} &=&  R(S\binom{x}{y}) \\
  &=& \alpha_2 
    \left(\begin{array}{cc}
      \cos(\theta_2) & -\sin(\theta_2)\\
      \sin(\theta_2) & \cos(\theta_2)
    \end{array}\right) 
  \left(\alpha_1 
    \left(\begin{array}{cc}
      \cos(\theta_1) & -\sin(\theta_1)\\
      \sin(\theta_1) & \cos(\theta_1)
    \end{array}\right)\binom{x}{y} + \binom{w_1}{z_1}\right) + \binom{w_2}{z_2}\\
&=& \alpha_1\alpha_2 \left(\begin{array}{cc}
      \cos(\theta_1)\cos(\theta_2) -\sin(\theta_1)\sin(\theta_2)& 
      -\sin(\theta_1)\cos(\theta_2) -\cos(\theta_1)\sin(\theta_2)\\
      \cos(\theta_1)\sin(\theta_2) +\sin(\theta_1)\cos(\theta_2)& 
      -\sin(\theta_1)\sin(\theta_2) +\cos(\theta_1)\cos(\theta_2)\\
    \end{array}\right)\binom{x}{y}\\
    &&+
  \left(\begin{array}{cc}
      \alpha_2\cos(\theta_2)w_1 -\alpha_2\sin(\theta_2)z_1 + w_2 \\
      \alpha_2\sin(\theta_2)w_1 +\alpha_2\sin(\theta_2)z_1 + z_2 \\
    \end{array}\right) \\
    &=& 
  \alpha_1\alpha_2\left(\begin{array}{cc}
      \cos(\theta_1 + \theta_2) & -\sin(\theta_1 + \theta_2)\\
      \sin(\theta_1 + \theta_2) & \cos(\theta_1 + \theta_2)
    \end{array}\right)\binom{x}{y} +
  \left(\begin{array}{cc}
      \alpha_2\cos(\theta_2)w_1 -\alpha_2\sin(\theta_2)z_1 + w_2 \\
      \alpha_2\sin(\theta_2)w_1 +\alpha_2\cos(\theta_2)z_1 + z_2 \\
    \end{array}\right) \\
\end{eqnarray*}
Which is also in \simtwo.
This formulation shows that for $R$ to be $S^{-1}$, we need $R$'s parameters to be
\begin{eqnarray*}
  \theta_2 &=&  -\theta_1\\
  w_1 &=& \alpha_1^{-1}(\sin(-\theta_1)z_1 - \cos(-\theta_1)w_1)\\
  z_1 &=& \alpha_1^{-1}(\cos(-\theta_1)z_1 + \cos(-\theta_1)w_1)\\
  \alpha_2 &=&  \alpha_1^{-1}
\end{eqnarray*}
Substituting this into the above equality gives $(R \circ S) \binom{x}{y} = \binom{x}{y}$, the identity element, so $R = S^{-1}$
\subsubsection{(b)}
We're looking at a subspace of $\reals^9$, such that there are only four distinct parameters.
We can therefore express any element of $\mathfrak{sim}(2)$ as
\[
\lambda \left(\begin{array}{ccc}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 0
    \end{array}\right) 
 + \theta \left(\begin{array}{ccc}
      0 & -1 & 0\\
      1 & 0 & 0\\
      0 & 0 & 0
    \end{array}\right) 
 + u\left(\begin{array}{ccc}
      0 & 0 & 1\\
      0 & 0 & 0\\
      0 & 0 & 0
    \end{array}\right) 
 + v\left(\begin{array}{ccc}
      0 & 0 & 0\\
      0 & 0 & 1\\
      0 & 0 & 0
    \end{array}\right) 
\]
This makes it clear that the four matrices span $\mathfrak{sim}(2)$; they are also clearly linearly independent.
We therefore have a simple bijection between $\mathfrak{sim}(2)$ and $\reals^4$.
Thus $\mathfrak{sim}(2)$ is isomorphic to $\reals^4$.
\subsubsection{(c)}
If we let follow the suggestion and let $\Omega$ and $J$ be defined as in the assignment,
we can verify that $J^2 = -I$ by high technology.
Now, we need to show this by induction on $k$:
\[ \Omega^k = \frac{1}{2}\left((\lambda + i\theta)^k + (\lambda + i\theta)^k \right)I
  + \frac{1}{2i}\left((\lambda + i\theta)^k - (\lambda + i\theta)^k \right) J\]
We start with a base case of $k=0$, using the convention that $A^0 = I$ for any square matrix $A$:
For the base case, $\Omega^0 = I$, so we just need to show that the expression evaluates to $I$.
\[
  \frac{1}{2}\left((\lambda + i\theta)^0 + (\lambda + i\theta)^0 \right)
  + \frac{1}{2i}\left((\lambda + i\theta)^0 - (\lambda + i\theta)^0 \right) = \frac{1}{2} (2I) + \frac{1}{2i}(0)J = I
\]

We now assume that that this equality holds for $k$, and try to show tha it is also true for $k+1$.
\begin{eqnarray*}
\Omega^{k+1} &=&  \Omega^k \cdot \Omega\\
&=& \left( \frac{1}{2}\left((\lambda + i\theta)^k + (\lambda + i\theta)^k \right)I
  + \frac{1}{2i}\left((\lambda + i\theta)^k - (\lambda + i\theta)^k \right)J \right)(\lambda I + \theta J)\\
  &=& \frac{1}{2}\lambda\left( (\lambda + i\theta)^k + (\lambda - i\theta)^k \right)I  
     - \frac{1}{2i}\theta\left( (\lambda - i\theta)^k - (\lambda - i\theta)^k \right)I +\\
  &&  \frac{1}{2i}\lambda\left( (\lambda + i\theta)^k + (\lambda - i\theta)^k \right)J +
      \frac{1}{2}\theta\left( (\lambda + i\theta)^k + (\lambda - i\theta)^k \right)J\\
  &=& \frac{1}{2}\left( \lambda(\lambda + i\theta) + \lambda(\lambda - i\theta)^k +
                        i\theta(\lambda + i\theta)^k - i\theta(\lambda-i\theta)\right)I + \\
  && \frac{1}{2i}\left( \lambda(\lambda + i\theta) - \lambda(\lambda - i\theta)^k +
                        i\theta(\lambda + i\theta)^k + i\theta(\lambda-i\theta)\right)J\\
  &=& \frac{1}{2}\left((\lambda + i\theta)(\lambda + i\theta)^k +
                       (\lambda - i\theta)(\lambda - i\theta)^k \right)I + \\
  && \frac{1}{2}\left((\lambda + i\theta)(\lambda + i\theta)^k -
                       (\lambda - i\theta)(\lambda - i\theta)^k \right)J \\
  &=& \frac{1}{2}\left((\lambda + i\theta)^{k+1} + (\lambda - i\theta)^{k+1} \right)I
  + \frac{1}{2i}\left((\lambda + i\theta)^{k+1} - (\lambda + i\theta)^{k+1} \right) J
\end{eqnarray*}
With that done, we can now look at
\begin{eqnarray*}
  e^\Omega &=&  \sum_{k=0}^\infty\frac{1}{k!}\Omega^k\\
  &=& \sum_{k=0}^\infty \frac{1}{k!}\left[ \frac{1}{2}\left((\lambda + i\theta)^k + (\lambda - i\theta)^k \right)I
    +\frac{1}{2i}\left((\lambda + i\theta)^k - (\lambda + i\theta)^k \right)J \right]\\
    &=& \frac{1}{2}I\left( \sum_{k=0}^\infty \frac{1}{k!}(\lambda + i\theta)^k +
            \sum_{k=0}^\infty \frac{1}{k!}(\lambda - i\theta)^k\right)
    +\frac{1}{2i}J\left(\sum_{k=0}^\infty \frac{1}{k!}(\lambda + i\theta)^k - 
    \sum_{k=1}^\infty \frac{1}{k!}(\lambda + i\theta)^k \right)\\
    &=& \frac{1}{2} \left( e^{\lambda + i\theta} + e^{\lambda - i\theta}  \right)I +
       \frac{1}{2i} \left( e^{\lambda + i\theta} - e^{\lambda - i\theta}  \right)J \\
    &=& e^\lambda(\cos(\theta)I + \sin(\theta)J)\\
    &=& e^\lambda\twomatrix{\cos(\theta)}{-\sin(\theta)}{\sin(\theta)}{\cos(\theta)}
\end{eqnarray*}
Whew!
\subsubsection{(d)}
We'll do this first proof by induction.
The base case is verified by $B^1 = \twomatrix{\Omega}{U}{0}{0}$. And the inductive case is
\[B^{n+1} = B^n\cdot B = \twomatrix{\Omega^n}{\Omega^{n-1} U}{0}{0} \twomatrix{\Omega}{U}{0}{0}
          = \twomatrix{\Omega^{n+1}}{ \Omega^n U}{0}{0}\]

Now, 
\begin{eqnarray*}
  e^B &=&  I + \sum_{k=1}^\infty \frac{1}{k!}\twomatrix{\Omega^k}{\Omega^{k-1}U}{0}{0}\\
  %&=& I + \sum_{k=1}^\infty \twomatrix{I +\frac{1}{k!}\Omega^k}{\frac{1}{k!}\Omega^{k-1}U}{0}{0}\\
  &=& \twomatrix{I + \sum_{k=1}^\infty \frac{1}{k!}\Omega^k}{\sum_{k=1}^\infty \frac{1}{k!}\Omega^{k-1}U}{0}{1}\\
\end{eqnarray*}
The upper-left entry is just the definition of $e^B$, and the upper-right entry is
\[
\sum_{k=1}^\infty \frac{1}{k!}\Omega^{k-1}U = I + \sum_{k=2}^\infty \frac{1}{k!}\Omega^{k-1}U 
= I + \sum_{k=1}^\infty \frac{1}{(k+1)!}\Omega^{k}U \text{ as desired }
\]

\subsubsection{(e)}
Let's start by unfolding the definition of the integral.
\begin{eqnarray*}
  \int_0^1 e^{\Omega t}dt &=& \int_0^1 Idt + \int_0^1 \left( \sum_{k=1}^\infty \frac{t^k\Omega^k}{k!} \right) dt\\
  &=& I + \sum_{k=1}^\infty\left( \int_0^1 \frac{t^k}{k!}\Omega^k \right)
  = I + \sum_{k=1}^\infty\left( \Omega^k \int_0^1 \frac{t^k}{k!}\right)\\
  &=&  I + \sum_{k=1}^\infty\left( \Omega^k \left[\frac{t^{k+1}}{(k+1)!}\right]_0^1\right)
  = I + \sum_{k=1}^\infty \frac{\Omega^k }{(k+1)!} = V
\end{eqnarray*}
This, of course, depends on the power series of a matrix being uniformly convergent, since we switch the integral and summation on the second line.
I will assume that power series of matrices are uniformly convergent just as power series of complex numbers are, and that the result I'm relying on (that we can switch these operations) goes through without a hitch.

Then, if $\theta = \lambda = 0$, then $\Omega = 0$, so $V = I + \sum_{k=1}^\infty \frac{0^k}{(k+1)!} = I$.
\subsubsection{(f)}
Let $s \in \textbf{SIM}(2)$. Then we can construct a $B \in \mathfrak{sim}(2)$ such that $e^B = s$.
We set $\lambda = \log(\alpha)$ and $\theta_B = \theta_s$.
Finally, we just need to show that we can find $u_B, v_B$ such that $ \binom{u_s}{v_s} = VU = V\binom{u_b}{v_b}$.
This can be done so long as $V$ is not singular.
Since 
$\text{det}(V) = \left(\lambda(e^\lambda\cos(\theta) - 1) + e^\lambda \theta \sin(\theta)\right)^2 + 
 \left(\theta(-e^\lambda\cos(\theta) + 1) + e^\lambda \lambda \sin(\theta)\right)^2 $, we would need both of the squared terms to be zero.
 This gives \[ \lambda(e^\lambda\cos(\theta) - 1) + e^\lambda \theta \sin(\theta) = 0 \text{ and }
 \theta(-e^\lambda\cos(\theta) + 1) + e^\lambda \lambda \sin(\theta) = 0 \]
 We can rewrite these equations as \[ \lambda(e^\lambda\cos(\theta) - 1) = - e^\lambda \theta \sin(\theta) \text{ and }
 \theta(e^\lambda\cos(\theta) - 1) = e^\lambda \lambda \sin(\theta) \] respectively.
 Some rearranging (which relies on $\theta, \lambda \neq 0$) gives
 \[ e^\lambda\cos(\theta) - 1 = -\frac{\theta}{\lambda} e^\lambda \sin(\theta) \text{ and }
 e^\lambda\cos(\theta) - 1 = \frac{\lambda}{\theta}e^\lambda \sin(\theta) \] respectively.
 which can only be true when $\theta$ or $\lambda$ is 0.
 Thus $V$ must not be singular, meaning that there exists some $B$ such that
 $e^B = s$ by construuction, so $exp$ is surjective.

\subsection{B4}
\subsubsection{(a)} Checked. (I'm really not sure how to show any work, since the answer is in the assignment.)
\subsubsection{(b)}
We can express any element of $U$ as this linear combination of three linearly independent matrices:
\[ u_1\threematrix{0}{0}{0}{0}{0}{-1}{0}{1}{0} + 
   u_2\threematrix{0}{0}{1}{0}{0}{0} {-1}{0} {0} +
   u_3\threematrix{0}{-1}{0}{1}{0}{0}{0}{0}{0} \]
As we can see from setting $u_1 = u_2 = u_3 = 0$, this subspace of $M_3$ is a 3-dimensional vector space of real numbers, namely $\reals^3$.

We know thate these matrices are never invertible, since we can always find a nontrivial kernel (as happens in the next line).
Or we could calculate the determinant to 0, since $-u_3u_1u_2 + u_2u_3u_1 = 0$.

The kernel can be found by looking at (a), where we notice that each entry is $0$ precisely when $u_1 = v_1, u_2 = v_2, u_3 = v_3$.
(The kernel is one-dimensional, since row-reduction shows that $U$ has dimension 2).
Thus the kernel is ${\alpha(u_1, u_2, u_3) | \alpha \in \reals}$.
The linear map $U$ rotates its input $(v_1, v_2, v_3)$ to be orthogonal to both $(u_1, u_2, u_3)$ and $(v_1, v_2, v_3)$, and multiplies the length of the input vector by the length of $(u_1, u_2, u_3)$ and product of the sine of the angle between $(u_1, u_2, u_3)$.

If $(u_1, u_2, u_3) \cdot (v_1, v_2, v_3) = 0$, then $UV = $ \textbf{TODO!}

\subsection{(B5)}
\subsubsection{(a)}
\begin{eqnarray*}
  A^2 &=& 
  \left( \begin{array}{ccc}
  -b^2 - c^2 & ab & ac\\
  ab & -a^2 - c^2 & bc\\
  ac & bc & -a^2 - b^2
  \end{array}\right) = 
  \left( \begin{array}{ccc}
  a^2 -(a^2 + c^2 + b^2) & ab & ac\\
  ab & b^2 -(a^2 + c^2 + b^2)& bc\\
  ac & bc & c^2 -(a^2 + c^2 + b^2)
  \end{array}\right) \\
  &=& -\threematrix{(a^2+b^2+c^2)}{0}{0}{0}{(a^2+b^2+c^2)}{0}{0}{0}{(a^2+b^2+c^2)}
  + B = -\theta^2 + B
\end{eqnarray*}
When computing $AB$ and $BA$, each entry ends up being $abc - abc = 0$, so $AB = 0$.
Then, \[A^3 = A\cdot A^2 = A(-\theta^2I + B) = -\theta^2A + AB = -\theta^2A\].

\subsubsection{(b)}
\begin{eqnarray*}
  e^A &=&  I + \sum_{k=1}^\infty \frac{A^k}{k!} = 
  I + A + \frac{A^2}{2!} + \frac{A^3}{3!} + \frac{A^4}{4!} + \frac{A^5}{5!} + \dots \\
  &=& I + A + \frac{A^2}{2!} -\theta^2 \frac{A}{3!} -\theta^2 \frac{A^2}{4!} + \theta^4 \frac{A}{5!} +  \theta^4 \frac{A^2}{6!} \dots \\
  &=& I + \left(A - \theta^2 \frac{A}{3!} + \theta^4 \frac{A}{5!} + \dots \right) + \left( \frac{A^2}{2!} - \theta^2 \frac{A^2}{4!}  \theta^4 \frac{A^2}{6!} + \dots \right)\\
  &=& I + A\left(1 - \theta^2 \frac{1}{3!} + \theta^4 \frac{1}{5!} + \dots \right) + A^2\left( \frac{1^2}{2!} - \theta^2 \frac{1^2}{4!}  \theta^4 \frac{1^2}{6!} + \dots \right)
 \end{eqnarray*}
 As in question B2, we note that 
$\frac{\sin(\theta)}{\theta} = 1 - \frac{\theta^2}{3!} + \frac{\theta^5}{5!} - \dots $.
And
$\frac{1 - \cos(\theta)}{\theta^2} = \frac{1}{2!} - \frac{\theta^2}{4!} + \frac{\theta^4}{6!} - \dots $

So we have $e^A = I + \frac{\sin(\theta)}{\theta}A + \frac{1-\cos(\theta)}{\theta^2}A^2$.
Given that did the \textsl{same exact computation} as in B2, we unsurprisingly got the same result!

\subsubsection{(c)}
Given that $A^3 = -\theta^2A$, we notice that $A^3 + \theta^2A = 0$.
Its characteristic polynomial happens to be $\lambda^3 + (a^2 + b^2 + c^2)\lambda = 0$, or $\lambda^3 + \theta^2\lambda = 0$.
This is easily solvable, and we find that $\lambda = 0, \pm i\theta$.

We can then find the eigenvalues of $e^A$ by pluggin in $A$'s eigenvalues to the formula for $e^A$.
This works because any eigenvector of $A$ is an eigenvector of $A^2$ (with the eigenvalue squared), and every vector is an eigenvector of $I$ with eigenvalue 1.

We therefore find that, for $\lambda=0$, we get the eigenvalue $1$;
for $\lambda=\pm i\theta$, we get
\[1 + \frac{\sin(\theta)}{\theta}(\pm i\theta) + \frac{1-\cos(\theta)}{\theta^2}(\pm i\theta)^2
= 1 \pm i\sin(\theta) - 1 + \cos(\theta) = \cos(\theta) \pm i\sin(\theta)
= e^{\pm i\theta}\]
And all three of these eigenvalues for $e^A$ have eigenvalue 1, which means that $e^A$ must be orthogonal, as it has a full set of distinct eigenvalues of unit length.

\subsubsection{(d)}
We'll just fill in the missing parts that aren't provided in the problem set.

\noindent (2)
To verify the formula for $\text{exp}^{-1}(R)$, we plug in $e^{\text{exp}^{-1}(R)}$ to get

\begin{eqnarray*}
e^{\text{exp}^{-1}(R)}&=& 
I + \frac{\sin(\theta)}{\theta} \frac{\theta}{2\sin(\theta)}(R - R^\top)
+ \frac{1-\cos(\theta)}{\theta^2} \left( \frac{\theta}{2\sin(\theta)}(R - R^\top) \right)^2
\end{eqnarray*}
To complete this, we use Theorem 10.13, as generously suggested by Professor Gallier.
Since $R$ is a rotation matrix, there exist some orthogonal matrix $Q$ and block diagonal matrix $D = \threematrix{\cos(\theta)}{-\sin(\theta)}{0}{\sin(\theta)}{\cos(\theta)}{0}{0}{0}{1}$ such that $R = QDQ^\top$.
Then \[R - R^\top = QDQ^\top - (QDQ^\top)^\top = QDQ^\top - QD^\top Q^\top
                  = Q(D - D^\top)Q^\top\]
Since $D - D^\top = \threematrix{0}{-2\sin(\theta)}{0}{\sin(\theta)}{0}{0}{0}{0}{0}$,
we also have $(D - D^\top)^2 = -\threematrix{4\sin^2(\theta)}{0}{0}{0}{4\sin^2(\theta)}{0}{0}{0}{0}$.
Note that $I = QIQ^\top$, so
\begin{eqnarray*}
e^{\text{exp}^{-1}(R)}&=& Q\left(
I + \frac{\sin(\theta)}{\theta} \frac{\theta}{2\sin(\theta)}(D - D^\top)
+ \frac{1-\cos(\theta)}{\theta^2} \left( \frac{\theta}{2\sin(\theta)}(D - D^\top) \right)^2\right)Q^\top\\
&=& Q\left(I + \threematrix{0}{-\sin(\theta)}{0}{\sin(\theta)}{0}{0}{0}{0}{0}
- \threematrix{1 - \cos(\theta)}{0}{0}{0}{1-\cos(\theta)}{0}{0}{0}{0}\right)Q^\top\\
&=& Q\threematrix{\cos(\theta)}{-\sin(\theta)}{0}{\sin(\theta)}{\cos(\theta)}{0}{0}{0}{1} Q^\top
 = QDQ^\top = R
\end{eqnarray*}

\noindent (3)
Using the factoring $R = QDQ^\top$ and the fact that eigenvalues are invariant under change of basis, we can read the eigenvalues of $R$ off of $D$ easily.
If we know that $-1 = \trace{D} = \cos(\theta) + \cos(\theta) + 1$, then we can easily tell that $\cos(\theta) = -1,$ so $\theta)$ is an odd integer multiple of $\pi$.
Therefore $\sin(\theta) = 0$ and $\cos(\theta) = 1$, so $D = \threematrix{-1}{0}{0}{0}{-1}{0}{0}{0}{1}$.
The desired properties of $D$ (its eigenvalues, being symmetrical, and squaring to $I$) can then all be read straight off the matrix!
Since $Q$ is orthogonal, conjugating by it does not change symmetry or eigenvalues, and $QD^2Q^\top = QIQ^\top = QQ^\top = I$ shows that $R^2 = I$.

Then $S = Q\frac{1}{2}(D - I)Q^\top = Q\threematrix{-1}{0}{0}{0}{-1}{0}{0}{0}{0}Q^\top$, which immediately shows that $S$ is symmetric and is diagonalized by $Q$.

If we now consider $U$ as defined in the problem, we find that a $U^2=S$ has a solution by part (e), so such a $U$ must exist!

If $U^2 = S$, then the trace of $U^2$ should equal the trace of $S$, giving us 
$-2(b^2 + c^2 + d^2) = -2$, or $b^2 + c^2 + d^2 = 1$.
Finally, we note that, as $\theta$ was defined in the beginning of this question, $\theta = (2k+1)\pi \sqrt{b^2 + c^2 + d^2} = (2k+1)\pi\sqrt{1} = (2k+1)\pi$. So,
\begin{eqnarray*}
  exp(exp^{-1}(R)) &=&  I + \frac{\sin(\theta)}{\theta}(\theta U) + \frac{1-\cos(\theta)}{\theta^2} (\theta U)^2\\
  &=&  I + 0 + \frac{2}{\theta^2}(\theta^2 U^2) \\
  &=&  I + 2\frac{1}{2}(R - I) = R
\end{eqnarray*}
as desired.
\subsubsection{(e)}
This part submitted as a program.

\subsection{(B6)}

\subsubsection{(a)} First to show surjectivity, let $v \in \mathbb{R}^3$ be any vector. Then set $v^* = R^{-1} (v - W)$. Clearly $v^* \in \mathbb{R}^3$ since $W \in \mathbb{R}^3$ and $R^{-1}$ exists and has full rank, being a rotation matrix. Then

\[ \rho(v^*) = R v^* + W = R (R^{-1} (v - W)) + W = (v - w) + W = v \]

So $v \in Im \, (\rho) $

And if $\rho(v_1) = \rho(v_2)$, then
\[ Rv_1 + W = Rv_2 + W \Longrightarrow v_1 = v_2 \]

since we just subtract the $W's$ and multiply by $R^{-1}$. So $\rho$ is injective. 

Now we use the matrix representation to show that $\rho$ is a group, where the group multiplication is just matrix multiplication of the representative matrices of $\rho$, which is composition of $\rho$ . Let $\rho_1 \sim \left(\begin{array}{cc} R_1 & W_1 \\ 0 & 1 \end{array}\right)$ and $\rho_2 \sim \left( \begin{array}{cc} R_2 & W_2 \\ 0 & 1 \end{array}\right)$. Then 

\[\rho_1 \circ \rho_2 \sim \left(\begin{array}{cc} R_1 & W_1 \\ 0 & 1 \end{array}\right) \left( \begin{array}{cc} R_2 & W_2 \\ 0 & 1 \end{array}\right) = \left(\begin{array}{cc} R_1 R_2 & R_1 W_2 + W_1 \\ 0 & 1 \end{array}\right)\]

where we note that we can put the product of the blocks $R_1, R_2$ in the product matrix since the first three columns of the last row are all 0, so thus we just get rows of $R_1$ multiplied by columns of $R_2$. And similarly in computing the top right term we see that it is the sum $(R_1)_{i*} (W_2)_i + (W_1)_i$ over $i=1,2,3$, where $(R_1)_{i*}$ is the $i$th row of $R_1$. 

Since $R_1 R_2$ is still an orthogonal matrix, as orthogonal matrices form a group under multiplication, and $R_1 W_2 + W_1 \in \mathbb{R}^3$, then this is the representation of an affine map with $R = R_1 R_2$ and $W = R_1 2 + W_1$

Thus affine maps are closed under composition. 

The matrix $\left(\begin{array}{cc} R & W \\ 0 & 1 \end{array}\right)$ has determinant $|R| = 1$; we can see this by just doing the determinant expansion on the last row from the right. Thus this matrix is invertible, so it has an inverse (which must be a matrix of the same form by the closure property earlier). 

And the group identity is just the identity matrix formed by setting $R = I_2$ and $W = 0$. 

So this forms a group.

\subsubsection{(b)} 

Let $\vec{W} = (x,y,z)$ be a component decomposition. Then for a matrix $B$, we define the isomorphism  in the canonical way by 
\[ \phi : (B, *) \longrightarrow (\mathbb{R}^6, +) \]
\[ \phi(B) \longrightarrow (a,b,c,x,y,z) \]

So adding two matrices of the form $B$ is just adding the parameters, and there are exactly six such parameters. We can choose them arbitrarily to represent any vector in $\mathbb{R}^6$. Injectivity of $\phi$ is obvious. Thus this is an isomorphism. 

For two matrices $B_1, B_2$ with corresponding parameters $(a_1,b_1,c_1,x_1,y_1,z_1)$ and $(a_2,\cdots ,z_2)$, we have
\[ B_1 * B_2 = \left( \begin{array}{cccc} 0 & -c_1 & b_1 & x_1 \\ c_1 & 0 & -a_1 & y_1 \\ -b_1 & a_1 & 0 & z_1 \\ 0 & 0 & 0 & 0 \end{array}\right) \left( \begin{array}{cccc} 0 & -c_2 & b_2 & x_2 \\ c_2 & 0 & -a_2 & y_2 \\ -b_2 & a_2 & 0 & z_2 \\ 0 & 0 & 0 & 0 \end{array}\right)\]

\[ = \left( \begin{array}{cccc} -c_1 c_2 - b_1 b_2 & b_1 a_2 & c_1 a_2 & -c_1 y_2 + b_1 z_2 \\ -a_1 b_2 & -c_1 c_2 - a_1 a_2 & c_1 b_2 & c_1 x_2 - a_1 z_2 \\ \cdots & \cdots & \cdots & \cdots \end{array}\right)\]

Well these are just the first two rows of the resulting product matrix, but we can already see that if, for example, $b_1 a_2 \neq b_2 a_1$, then the second entry of the first row will be different if we reverse the order of multiplication. Thus in general these matrices do not commute. 

\subsubsection{(c)} 

Let $B_1, B_2$ be two matrices of the form $B$, with $\Omega_1,W_1$ and $\Omega_2,W_2$ as the blocks. Then

\[ B_1 B_2 = \left(\begin{array}{cc} \Omega_1 \Omega_2 & \Omega_1 W_2 \\ 0 & 0 \end{array}\right) \]

where, similarly as in part (b), since the last row of $B_2$ is zero, then we are just multiplying the first three columns of $B_1$ with the first three rows of $B_2$. So we get $\Omega_1 \Omega_2$ in the first 3-by-3 block of the product matrix, and then we get the first 3-by-3 block of $B_1$ multiplied by the last column of $B_2$, which is just $\Omega_1 W_2$.

Thus, we have 

\[ BB = \left( \begin{array}{cc} \Omega^2 & \Omega W \\ 0 & 0 \end{array}\right)\]

and inductively, if the formula holds for $B^{n-1}$, then

\[ B^{n-1} B = \left(\begin{array}{cc} \Omega^n & \Omega^{n-1} \\ 0 & 0 \end{array}\right) \]

Now we have 

\[ e^B = I_4 + \displaystyle\sum_{n=1}^{\infty} \dfrac{1}{n!} B^n = I_4 + \displaystyle\sum_{n=1}^{\infty} \dfrac{1}{n!} \left(\begin{array}{cc} \Omega^n & \Omega^{n-1}W \\ 0 & 0 \end{array}\right) = \left( \begin{array}{cc} e^{\Omega} & VW \\ 0 & 1 \end{array}\right)\]

with $V$ given by

\[V = \displaystyle\sum_{n=1}^{\infty} \dfrac{1}{n!} \Omega^{n-1} = \Omega^0  + \displaystyle\sum_{n=2}^{\infty} \dfrac{1}{n!} \Omega^{n-1} = I_3 + \displaystyle\sum_{k\ge 1} \dfrac{1}{(k+1)!} \Omega^k\]

where we just substitute $k=n-1$ to get the last expression. 

Now suppose $\sqrt{a^2 + b^2 + c^2} = 0$. Then we compute the powers of $\Omega$ to get: 

\[ \Omega^2 = \left(\begin{array}{ccc} -c^2 - b^2 & ab & ac \\ ab & -c^2 - a^2 & bc \\ ac & bc & -b^2 - a^2 \end{array}\right) = \left(\begin{array}{ccc} a^2 & ab & ac \\ ab & b^2 & bc \\ ac & bc & c^2 \end{array}\right) \]

\[ \Omega^3 = \left(\begin{array}{ccc} a^2 & ab & ac \\ ab & b^2 & bc \\ ac & bc & c^2 \end{array}\right) \left(\begin{array}{ccc} 0 & -c & b \\ c & 0 & -a \\ -b & a & 0 \end{array}\right) = \left(\begin{array}{ccc} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{array}\right) \]

Thus all powers $\Omega^n$ with $n > 2$ are 0, so we are left with
\[ e^{\Omega} = I_3 + \Omega + \dfrac{1}{2} \Omega^2 \]

which should just be $I_3$?

\subsubsection{(d)} 

For general $\theta$, we have

\[\Omega^3 = (a^2 + b^2 + c^2) \left(\begin{array}{ccc} 0 & c & -b \\ -c & 0 & a \\ b & -a & 0 \end{array}\right) = -\theta^2 \Omega \]

\[\Omega^4 = -(a^2 + b^2 + c^2) \Omega^2 = -\theta^2 \Omega^2 \]

\[ \Omega^5 = \theta^4 \Omega\]

and so forth. thus we can break up the exponential according to the parity of the power. We get

\[e^{\Omega} = I_3 + \displaystyle\sum_{n=1}^{\infty} \dfrac{1}{(2n-1)!} (-\theta^2)^{n-1} \Omega + \displaystyle\sum_{n=1}^{\infty} \dfrac{1}{(2n)!} (-\theta^2)^{n-1} \Omega^2\]

and we can rewrite the coefficients of $\Omega, \Omega^2$ as 

\[\dfrac{\Omega}{\theta} \displaystyle\sum_{n=1}^{\infty} \dfrac{(-1)^{n-1}}{(2n-1)!} \theta^{2n-1} + \dfrac{\Omega^2}{\theta^2} \displaystyle\sum_{n=1}^{\infty} \dfrac{1}{(2n)!} (-1)^{n-1} \theta^{2n} \] 

the first summation is the Taylor series expansion of $\sin \theta$ and the latter summation is the negative of the Taylor series expansion of $\cos \theta$ except for the first term, or $1 - \cos \theta$. 

So we get the desired 

\[ e^{\Omega} = I_3+ \dfrac{\sin \theta}{\theta} \Omega + \dfrac{1 - \cos \theta}{\theta^2} \Omega^2 \]

We can do the same thing for $V$:

\[ \displaystyle\sum_{k\ge 1} \dfrac{1}{(k+1)!} \Omega^k = \displaystyle\sum_{k = 1}^{\infty} \dfrac{1}{(2k)!} \Omega^{2k-1} + \displaystyle\sum_{k=1}^{\infty} \dfrac{1}{(2k+1)!} \Omega^{2k} = \Omega \displaystyle\sum_{k = 1}^{\infty} \dfrac{1}{(2k)!} (-\theta^2)^{k-1} + \Omega^2 \displaystyle\sum_{k=1}^{\infty} \dfrac{1}{(2k+1)!} (-\theta^2)^{k-1} \]

\[ = - \dfrac{\Omega}{\theta^2} \displaystyle\sum_{k=1}^{\infty} \dfrac{(-1)^k}{(2k)!} \theta^{2k} + - \dfrac{\Omega^2}{\theta^3} \displaystyle\sum_{k=1}^{\infty} \dfrac{(-1)^k}{(2k+1)!} \theta^{2k+1} = \dfrac{1-\cos \theta}{\theta^2} \Omega + \dfrac{\theta - \sin \theta}{\theta^3} \Omega^2\]

and so 

\[V = I_3 + \dfrac{1-\cos \theta}{\theta^2} \Omega + \dfrac{\theta - \sin \theta}{\theta^3} \Omega^2\]

\subsubsection{(e)}

We will show that the matrix representation of $e^B$ has the form $\left( \begin{array}{cc} R & W \\ 0 & 1 \end{array}\right)$. Recall that $e^B = \left(\begin{array}{cc} e^{\Omega} & VW \\ 0 & 1 \end{array}\right)$. $VW$ is clearly a vector in $\mathbb{R}^3$, so we just need to show that $e^{\Omega}$ is orthogonal. 

Note that $\Omega^T = -\Omega$ and $(\Omega^2)^T = \Omega^2$. Thus we have 

\[ (e^{\Omega})^T = I_3 + \dfrac{\sin \theta}{\theta} \Omega^{-1} + \dfrac{1 - \cos \theta}{\theta^2} \Omega^2 \]

and so 

\[ (e^{\Omega})^T e^{\Omega} = \left(I_3 + \dfrac{\sin \theta}{\theta} \Omega + \dfrac{1 - \cos \theta}{\theta^2} \Omega^2 \right) \left(I_3 - \dfrac{\sin \theta}{\theta} \Omega + \dfrac{1 - \cos \theta}{\theta^2} \Omega^2 \right) \]

\[ = I_3 - \dfrac{\sin^2 \theta}{\theta^2} \Omega^2 + \dfrac{1 - 2\cos \theta + \cos^2 \theta}{\theta^4} (-\theta^2) \Omega^2  + 2\dfrac{1 - \cos \theta}{\theta^2} \Omega^2\]

\[ = I_3 - \dfrac{2 - 2 \cos \theta}{\theta^2} \Omega^2 + \dfrac{2 - 2 \cos \theta}{\theta^2} \Omega^2 = I_3\]

So $(e^{\Omega})^T = (e^{\Omega})^{-1}$; thus it is orthogonal, and so $e^B$ is an affine map. 

Showing that $V$ is invertible and that the inverse is $Z$ is just a matter of multiplying the expression given for $Z$ by $V$ and showing the product is $I$ using elementary algebra and the established identities. 

Since $V$ is invertible, then $V$ acts transitively on $\mathbb{R}^3$, so $VW$ for $W \in \mathbb{R}^3$ can any arbitrary vector by choosing an appropriate $W$. It remains to show that $e^{\Omega}$ is any arbitrary rotation matrix. 

We know that it is a linear transformation, so it is completely determined by its action on the basis vectors of $\mathbb{R}^3$. So let $e^{\Omega} e_1 = v_1, e^{\Omega} e_2 = v_2, e^{\Omega} e_3 = v_3$. 

Now from before we have 

\[ e^{\Omega} = I_3+ \dfrac{\sin \theta}{\theta} \Omega + \dfrac{1 - \cos \theta}{\theta^2} \Omega^2 \]

This is a linear map except for the $\theta$ terms; however we note that the coefficients involving $\theta$ are continuous functions of the parameters. And so this gives is three parameters to vary, and we can just adjust them so that each basis vector is sent to the correct location. 

\subsection{(B7)}

\subsubsection{(1)}
The eigenvalues of $AA^*$ are given by det $(xI_2 - AA^*) = x^2 - $ tr$(AA^*) + $ det$(AA^*)$. Since det$(A) = $ det$(A^*)$ then this $x^2 - $tr$(AA^*) + |$det$(A)|^2$. And so the singular values are the roots of this equation. 

\subsubsection{(2)}
We have 
\[AA^* = \left(\begin{array}{cc} a_{11}^2 + a_{12}^2 & a_{11} a_{21} + a_{12} a_{22} \\ a_{11} a_{21} + a_{12} a_{22} & a_{21}^2 + a_{22}^2 \end{array}\right) \]

Now the two roots $\sigma_1, \sigma_2$ add to $a_{11}^2 + a_{12}^2 + a_{21}^2 + a_{22}^2$ and multiply to $|a_{11} a_{22} - a_{12} a_{21}|^2$.  So 

\[ \mu(A) = \dfrac{\sigma_1 + \sigma_2}{2\sqrt{\sigma_1 \sigma_2}} \]

and thus

\[ \sqrt{\mu(A)^2 - 1} = \sqrt{\dfrac{\sigma_1^2 + \sigma_2^2 + 2\sigma_1 \sigma_2}{4 \sigma_1 \sigma_2} - 1} = \sqrt{\dfrac{(\sigma_1 - \sigma_2)^2}{4 \sigma_1 \sigma_2}} = \dfrac{\sigma_1 - \sigma_2}{2\sqrt{\sigma_1 \sigma_2}}\]

\[ \mu(A) + \sqrt{\mu(A)^2 - 1} = \dfrac{\sigma_1}{\sqrt{\sigma_1 \sigma_2}} = \sqrt{\dfrac{\sigma_1}{\sigma_2}} \]

Now the eigenvalues of $A^{-1}$ are the inverses of the eigenvalues of $A$, since if $Av = \lambda v$ for eigenvalue $\lambda$, by the definition of inverse $A^{-1} A v = v$ so $A^{-1} v = \dfrac{1}{\lambda} v$. The spectral norm of $A$ is the larger of the two eigenvalues of $A$, and after inverting the eigenvalues the smaller eigenvalue of $A$ becomes the larger eigenvalue of $A^{-1}$, so thus the condition number is the quotient of the eigenvalues of $A$. Not sure why I get a square root...

\subsubsection{(3)}

\[ \dfrac{d}{d\mu(A)} cond_2(A) = 1 + \dfrac{2 \mu(A)}{\sqrt{\mu(A)^2 - 1}} \]

So if $\mu(A) > 1$ for all $A$ in this set, then $cond_2(A)$ is a monotonically increasing function of $\mu(A)$ and so they share the same maximums. But $\mu(A)$ is unbounded in $S$, so I don't see how this maximum is well defined over all of $S$. We need the additional restriction that $\det(A) \neq 0$. 

For the given $A_m$, 

\[ \mu(A_m) = \dfrac{100^2 + 99^2 + 99^2 + 98^2}{2|100*98 - 99^2|} = 19603; \det(A_m) = (99+1)(99-1) - 99^2 = -1 \]

\[ cond_2(A_m) = 19603 + \sqrt{19603^2 - 1} = 39205.9999745 \approx 39206 \]

\subsubsection{(4)}

The maximum value of the numerator is clearly when each term is 100, which gives $4 * 100^2 = 4 * 10000$. If $|\det(A)| \ge 2$ then $2|a_{11} a_{22} - a_{12} - a_{21}| \ge 4$ and thus $\mu(A) \le \dfrac{4 * 10000}{4} = 10000$. 

Since $\det(A) \neq 0$, then to show that $\mu(A)$ is maximized for $\det(A) = \pm 1$ it is sufficient to exhibit one such matrix with $\mu(A) > 10000$. And we already did ths in part (3). 

Set $n_1 = a_{11}, n_4 = a_{22}, n_2 = a_{12}, n_3 = a_{21}$. Then $|n_1 n_4 - n_2 n_3| = 1$ is the condition that $\det(A) = \pm 1$. $0 \le n_4 \le \cdots \le n_1 \le 100$ is just the condition of $A$ being in the set $S$. Furthermore we know the matrix $A^* = \left(\begin{array}{cc} 100 & 99 \\ 99 & 98 \end{array}\right)$ exists and has the specified $\mu$ value, so to maximize $\mu$ we can just consider matrices which might potentially have greater $\mu$ than that of $A_m$. Since the denominator in the expression of $\mu(A)$ is 2 after we fix the determinant, then if the numerator is less than $39206$, clearly $A_m$ has greater $\mu$ than $A$, so we do not need to consider such matrices when maximizing $\mu$. 

And so $\{100,99,99,98\}$ is the only solution to this system; thus it maximizes $\mu(A)$. 

\subsubsection{(5)} 

All of these matrices have the same entries modulo permutation, so $a_{11}^2 + \cdots + a_{22}^2$ is the same for all of them. Furthermore the have the same determinant modulo $\pm 1$. So they all have the same $\mu$ value. And so the conclusion follows. 

\subsubsection{(6)} 

\[A_m^{-1} = \left(\begin{array}{cc} -98 & 99 \\ 99 & -100 \end{array}\right) \Longrightarrow \left(\begin{array}{c} x_1 \\ x_2 \end{array}\right) = \left(\begin{array}{cc} -98 & 99 \\ 99 & -100 \end{array}\right) \left(\begin{array}{c} 199 \\ 197 \end{array}\right) = \left(\begin{array}{c} 1 \\ 1 \end{array}\right) \]

\[\delta x = \left(\begin{array}{cc} -98 & 99 \\ 99 & -100 \end{array}\right) \left(\begin{array}{c} -0.0097 \\ 0.0106 \end{array}\right) = \left(\begin{array}{c} 2 \\ -2.0203 \end{array}\right) \]

And so the solution to the perturbed system is 

\[ y = \left(\begin{array}{c} 3 \\ -1.0203 \end{array}\right) \]

\[ \|x\|_2 = \sqrt{2}, \|\delta x \|_2 = 2.8428, \|b\|_2 = 280.0179, \|\delta b\|_2 = 0.0144 \]

\[ c = \dfrac{2.8428}{\sqrt{2}} \dfrac{280.0179}{0.0144} = 39175.3 \approx 39206\]

\subsection{(B8)}

\subsubsection{(a)} The transformation from a basis to itself is just the identity matrix, which has determinant $1 > 0$.

If $U$ changes the basis from $\{u_1,\cdots,u_n\}$ to $\{v_1,\cdots,v_n\}$, then $U^{-1}$ changes the basis back the other way. And if $\det(U) > 0$, then since $\det(U) \det(U^{-1}) = \det(UU^{-1}) = 1 > 0$, we must have $\det(U^{-1}) > 0$. 

Finally if $U,V$ are two change of basis matrices with $\det(U)>0, \det(V)>0$, then $\det(UV)=\det(U)\det(V) > 0 $. So this is an equivalence relation. 

\subsubsection{(b)} Let $X$ be the matrix which changes $B_2$ into $B_1$. Since $X$ is an isomorphism of vector spaces, it must send basis vectors to basis vectors. Furthermore $B_1$ is an orthonormal basis, so these image vectors are orthonormal vectors. Let $\{e_1,\cdots,e_n\}$ be an orthonormal basis for $B_2$. Given two vectors $v_1 = \displaystyle\sum_i \alpha_i e_i$ and $v_2 = \displaystyle\sum_i \beta_i e_i$, we have

\[ <Xv_1, Xv_2> = <\sum_i \alpha_i X e_i, \sum_i \beta_i X e_i> = \sum_i \alpha_i \beta_i < Xe_i, X e_i> = \sum_i \alpha_i \beta_i\]

where we've used orthonormality of $Xe_i$'s to decompose the inner product. But this is equal to 

\[ <v_1, v_2> = \sum_i \alpha_a \beta_i \]

thus $X$ preserves inner products. 

Then for $i\neq j$:

\[ <X e_i, e_j> = <Xe_i, XX^{-1} e_j > = <e_i, X^{-1} e_j> \]

thus $X^{-1} = X^T$, so $X$ is an orthogonal operator. Hence we have $|\det X| = 1$, and since $B_1,B_2$ have the same orientation, $\det X = 1$. 

\[\det_{B_2} (w_1,\cdots,w_n) = \det_{B_1} X(w_1,\cdots,w_n) = \left (\det_{B_1} X\right) \left(\det_{B_1} (w_1,\cdots,w_n)\right) = \det_{B_1} (w_1,\cdots,w_n)\]

and so this is independent of basis. 

\subsubsection{(c)}

We can see that this is a linear map by computing the determinant via cofactor expansion on the last column. 

Now denote by $E_{n-1}$ the vector subspace spanned by $w_1,\cdots,w_{n-1}$. Let $e \in E - E_{n-1}$ be the basis vector spanning the dimension not spanned by $w_1,\cdots,w_{n-1}$. Then for any $x \in E$ we write it as $x = y + ae$ where $y \in E_{n-1}$ and $ae \in E - E_{n-1}$. Then $\lambda(w_1,\cdots,w_{n-1},y) = 0$ since we have $n$ vectors in an $n-1$ dimension space and so they must be linearly dependent. We are left with $\lambda(w_1,\cdots,w_{n-1},ae)$, which is now a function of one variable, namely the coefficient $a$. Furthermore by linearity of $\lambda$ we have 

\[ \lambda(w_1,\cdots,w_{n-1},ae) = a \lambda(w_1,\cdots,w_{n-1},e) \]

so thus 

\[ \lambda(w_1,\cdots,w_{n-1},x) = a \lambda(w_1,\cdots,w_{n-1},e) \]

now we define

\[ w_1 \times \cdots \times w_{n-1} \equiv \lambda(w_1,\cdots,w_{n-1},e) e\]

so we get the desired 

\[ <w_1 \times \cdots \times w_{n-1}, x> = < \lambda(w_1,\cdots,w_{n-1},e) e, x> = \lambda(w_1,\cdots,w_{n-1},e) <e, x> \]
\[= a \lambda(w_1,\cdots,w_{n-1},e) = \lambda(w_1,\cdots,w_{n-1},x)\]

thus we have existence. Uniqueness follows from the fact that $w_1 \times \cdots \times w_{n-1}$ must be a vector in $E - E_{n-1}$, since if it contained a component of $w_i$, then if we input an $x = w_i$, we would get a zero for $\lambda(w_1,\cdots,w_{n-1},x)$ but something nonzero from the inner product. And then the scalar in front of the vector is clearly unique. 


\subsection{(B10)}

\subsubsection{(1)-(2)} Programming.

\subsubsection{(3)} The sequence $A_k$ converges to an upper triangular matrix whose diagonal entries are the eigenvalues of the input $A$. For symmetric matrices, this converges to a diagonal matrix whose entries are the eigenvalues.

At each iteration, we change $A_k = Q*R$ to $A_{k+1} = R*Q$. This is equivalent to adding the commutator: 

\[[Q,R] = QR - RQ\]

\[A_k - [Q,R] = A_{k+1} \]

The commutator of a symmetric matrix is symmetric, and by 


\[[Q,R]^T = R^TQ^T - Q^TR^T = R^TQ^{-1} - Q^{-1}R^T \]

\[[Q,R] [Q,R]^T = QRR^TQ^{-1} - R^TQ^{-1}RQ - QRQ^{-1}R^T + RQQ^{-1}R^T \]





\end{document}
